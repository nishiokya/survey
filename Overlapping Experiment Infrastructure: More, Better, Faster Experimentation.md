# 論文情報・リンク
論文リンク： https://ai.stanford.edu/~ronnyk/2009controlledExperimentsOnTheWebSurvey.pdf
公開日時： Published online: 30 July 2008; Journal issue: February 2009 ￼ ￼
実装コード： 内部実装プラットフォーム「ExP」（Microsoft内部・非公開）を紹介 ￼
Publication : Data Mining and Knowledge Discovery, Vol. 18, No. 1, pp. 140–181 (2009) ￼

# Abstract

制御実験（ランダム化実験、A/Bテストおよびその拡張、スプリットテスト、Control/Treatmentテスト、多変量テスト（MVT）、並行フライトとも呼ばれる）は、変更とユーザーの観察可能な行動への因果関係を確立するための、最も理にかなった科学的デザインを体現しています。本稿では、エンドユーザーの反応を活かして機能開発を導くオンライン実験の実践的ガイドを提供します。私たちの経験から、開発チームが「Highest Paid Person’s Opinion（HiPPO）」ではなく顧客の声に耳を傾けることで、大きな学びと投資収益率（ROI）が得られることが明らかになっています。

本稿では、驚くべき結果をもたらしたいくつかの制御実験事例を紹介するとともに、実験を実行する上で必要な重要要素と、その技術的・組織的な制約について論じます。特に、統計的検出力、サンプルサイズ、分散削減のための手法といった、実験設計における要所に焦点を当てています。

また、実験システムの一般的なアーキテクチャを説明し、それぞれの利点・欠点を分析します。さらに、実際には単純とは言えないランダム化およびハッシュ化技術についても評価を行います。

制御実験は通常、大量のデータを生み出しますが、これをデータマイニング技術で分析することで、結果に影響を与える要因を深く理解し、新たな仮説を生み出し、改善の好循環を生み出せます。明確な評価基準のもとで制御実験を取り入れる組織は、自動最適化やリアルタイム分析を通じてシステムを継続的に進化させることが可能です。

私たちは複数のシステムや組織での豊富な実践経験に基づき、信頼性の高い制御実験を実施するための重要な教訓を本稿で共有します。


# 1 Introduction
「千の専門家の意見に勝るは、一つの正確な測定である」
― アドミラル・グレース・ホッパー
1700年代、ある英国海軍の船長は、地中海方面の艦隊に配属された水兵が壊血病（スコルブ）の症状をほとんど示さないことに気づきました。彼らの配給食には柑橘類が含まれていたのです。そこで船長は乗組員を二つのグループに分け、一方（Treatment 群）にはライムを与え、もう一方（Control 群）には従来通りの食事を続けさせる実験を行いました。Treatment 群では不満の声も上がりましたが、実験は成功し、ライムの摂取が壊血病の予防に効果的であることが示されました。船長自身は壊血病がビタミンC欠乏によるものであることや、ライムに豊富にビタミンCが含まれていることを理解していませんでしたが、この介入は確実に功を奏しました。やがて英国水兵には定期的に柑橘類を摂取することが義務づけられ、その習慣が「ライミーズ（limeys）」という愛称を生むに至りました（Rossi et al. 2003; Marks 2000）。

それから約300年後、Amazon のグレッグ・リンデンは、ショッピングカートに入れた商品の履歴に応じてパーソナライズされた推薦を表示するプロトタイプを開発しました（Linden 2006a, b）。商品を一つ追加すると推薦が現れ、別の商品を追加すると別の推薦が表示される仕組みです。このアイデアは一見有望でしたが、あるマーケティング上級副社長は「チェックアウトの妨げになる」と断固反対し、リンデンは「これ以上手をつけるな」と命じられました。しかし、彼はそれでも制御実験を実施し、その機能は圧倒的な差で勝利を収め、「未導入のままでいることが Amazon にかなりの損失を与えている」ことが明らかになりました。急遽ショッピングカート推薦機能が本番導入され、その後、多くのサイトが同様の仕組みを採用しています。

本論文の著者たちは、Amazon、Microsoft、DuPont、NASA などで数多くの実験に携わってきました。特に Amazon では「直感よりデータを重視する」という文化と、実験を容易に行えるシステムが整備されており、迅速かつ効果的なイノベーションを可能にしています。Microsoft においても複数の制御実験システムが運用されており、本論文ではそれぞれのアーキテクチャの利点と欠点を解説します。本稿の一貫したテーマは、制御実験が優れた投資収益率（ROI）を生み出し、適切なインフラストラクチャを構築することでイノベーションのスピードを加速できるという点です。Stefan Thomke の著書『Experimentation Matters』（Thomke 2003）のタイトルは、その趣旨を端的に示しています。

Webは、制御実験（ランダム化実験：単因子設計または因子設計）、A/Bテスト（およびその一般化）、スプリットテスト、Control/Treatmentテスト、並行フライトといった手法を用いて、アイデアを迅速に評価するための前例のない機会を提供します。これらの実験の最もシンプルな形では、実際のユーザーをランダムに二つのバリアントのいずれかに割り当てます。(i) Control（通常は「既存」バージョン）、(ii) Treatment（評価対象となる新バージョン）です。実行時パフォーマンスから、暗黙的・明示的ユーザー行動、アンケートデータまで、多様な指標を収集し、統計検定を適用して、両バリアント間に有意差があるかどうかを判断します。これにより、「バージョン間に差はない」という帰無仮説を維持するか棄却するかを決定できます。さらに、OLAPなどの手動分析や機械学習・データマイニング技術によってユーザーセグメントを掘り下げることで、どのサブポピュレーションにおいて差が顕著かを明らかにし、アイデア理解を深め、次のステップへ進む助けとします。

制御実験は、アイデアを確実に評価するための方法論を提供します。事後分析や中断時系列分析（準実験法）などとは異なり、本手法は因果関係を検証するための実験デザインです（Keppel et al. 1992, pp. 5–6）。多くの組織には多彩なアイデアがありますが、そのROI（投資収益率）が不明瞭だったり、評価コストが高かったりします。しかし次節で示すように、わずかな変更が大きな違いを生むことがあり、その結果はしばしば予測を超えます。本番環境での実験は、アイデアの価値を見極めるうえで強力な指針を与えてくれます。

本稿の貢献は以下のとおりです。
- 第3節では、Web環境における制御実験の概要をレビューし、統計的検出力やサンプルサイズといった、初学者向け解説ではしばしば省略されがちな重要項目を含む豊富な文献を紹介します。さらに実務で有効だった分散削減手法、実践上の拡張・制約も論じ、実務者が陥りやすい落とし穴を回避できるようにします。
- 第4節では、オンライン環境における多変量テスト（MVT）の代替策を複数提示します。ソフトウェア領域では、従来型MVTよりも並行した一因子テストのほうが適している場合がある理由を論じます。
- 第5節では、著者が関与してきた複数の実験システムを統合する汎用アーキテクチャを示し、その長所と短所を比較します。また、実際には統計的妥当性を担保する条件付き独立性テストに通らないランダム化・ハッシュ化スキームが存在することを明らかにします。
-第6節では、実務で得られた重要な教訓を共有します。

企業が実験基盤を構築すれば、テストコストや実験失敗のダメージは小さくなり、実験によるイノベーションが促進されます。「早く失敗する」ことで、アイデアの有効性を迅速に見極め、より成功し得る次のアイデアへと舵を切ることが可能になるのです。


# 2 Motivating examples

「事実が少なければ少ないほど、意見は強くなる」
― アーノルド・グラスオウ

以下の事例は、複数の領域で驚くべき結果をもたらしています。
	1.	最初の２つは、ほんの小さなUI変更が劇的な差を生んだ例です。
	2.	3番目の事例では、広告による短期的な収益向上とユーザー体験の劣化とのトレードオフを、制御実験でどのように評価・最適化したかを示します。
	3.	4番目の事例は、バックエンドアルゴリズム（この場合はAmazonの検索機能）に制御実験を適用した例です。

 ## 2.1 Checkout page at Doctor FootCare

 ECサイトのコンバージョン率とは、サイト訪問のうち購入に至った割合を指します。以下の例は Bryan Eisenberg の記事（Eisenberg 2003a, b）からのものです。

⸻

どちらのバリアントがより高いコンバージョン率を示すと思いますか？また、その差は統計的に有意だと思いますか？

図1に示す Doctor FootCare のチェックアウトページには、バリアントAとバリアントBで計9箇所の違いがあります。デザイナーがこれらを見せて「どちらを本番に出すべきか？」と尋ねられたら、あなたは正しく予測できるでしょうか？差がどれくらいあるか、またその差が有意かを見積もることができるでしょうか？

読者の皆さんにも、答えを読む前にぜひ考えてみてほしいと思います。どちらのバリアントが良いか、どの程度の差があるか、ぜひ予測してみてください。予測がいかに難しいかを実感できるはずです。

⸻

【実験結果】
バリアントAはバリアントBを10倍以上上回りました。実際にはサイト運営者がAからBに切り替えた結果、売上が90%も減少しています。ほとんどの変更点は良い方向でしたが、クーポンコード欄が致命的でした。ユーザーが「割引クーポンがあるのに自分が持っていないのでは？」と考え直してしまったためです。新バージョン（B）からクーポンコード欄を削除したところ、旧バージョン（A）と比べてコンバージョン率が6.5%向上しました。

## 2.2 Ratings of Microsoft Office help articles
Microsoft Office のヘルプを利用するユーザー（あるいは Office Online サイト（http://office.microsoft.com）を閲覧するユーザー）は、閲覧した記事を評価する機会が与えられます。初期実装では Yes/No ウィジェットが提示されていましたが、チームはこれを 5 つ星評価ウィジェットへと変更しました。

変更の動機
	1.	5 つ星評価は、より細かいフィードバックを提供し、コンテンツ制作者の評価に役立つ可能性がある。
	2.	5 つ星評価は、Yes/No 用と「理由」入力用の２つのポップアップを出す代わりに、単一のフィードバックボックスを提示することで、ユーザビリティが向上するはずである。

どちらのウィジェットの応答率※が高かったか予測できますか？
※ここで応答率とは、ウィジェットに対する何らかの操作（クリックや評価入力）を行った割合とする

驚くべきことに、5 つ星評価に切り替えたところ評価数は約 90％ 減少し、期待したユーザビリティ向上（動機 #2）を大きく逸脱しました。さらなるテストにより、２段階モデル（まず Yes/No 的な問い、次に星評価）を採用すると応答率が改善することが判明しました。具体的には、図 3 のように「Not helpful」から「Very helpful」までの 5 段階を明示した２段階モデルのウィジェットは、図 4 のシングルステージ型と比べ、応答率が 2.2 倍 に向上しました。

また、動機 #1 の「より細かいフィードバック」についても、多くのユーザーは 1 または 5 の極端な評価を選択し、中間の星をほとんど使いませんでした。助けが必要な状況では、「役に立ったか」「役に立たなかったか」の二択になりがちだったのです。

最終的にチームは、yes/no/I-don’t-know（わからない） という三択モデルを採用しました。この方式は単純な yes/no より応答率はやや下がったものの、追加の「わからない」という選択肢がもたらす情報が有用と判断されました。

## 2.3 MSN home page ads
多くのサイト運営者が直面する重要な問いは、広告をどれだけ掲載すべきかという点です。短期的には広告枠を増やすことで収益が向上しますが、特にターゲット外の広告の場合、それがユーザー体験にどのような影響を与えるかを見極めるのは難しいトレードオフです。この問いに直面したのが、2007年末のMicrosoft MSNホームページチームでした。

MSNホームページはモジュール構成になっており、ショッピングモジュールは「ファーストビュー」の右側に表示されています。提案では、このモジュールのすぐ下にさらに3件のオファーを追加するというものでした（図5）。ほとんどのユーザーにとって、それらは折り返し後の位置に表示されることになります。表示広告マーケティングチームの試算では、これらの追加オファーだけで一日数万ドルの収益が見込めるとされました。

ここでのチャレンジは、「広告収益」と「ユーザー体験の劣化」をどう比較・評価するかです。本稿第3.1節で取り上げた「OEC（Overall Evaluation Criterion：総合評価基準）」の事例です。今回は、ページビュー数とクリック数の変化を調べ、それぞれに金銭的価値を割り当てることにしました。（実験期間中、訪問頻度に統計的有意な変化は見られませんでした。）
- ページビューの価値：MSNホームページで表示される広告から得られる収益に基づく。
- クリックの価値：
1. MSNホームページから他のMSNネットワークサイト（MSN Autos、MSN Moneyなど）へ遷移したクリックに対し、その宛先サイトが評価する金銭的価値。これにより複数のページビューが生成される。
2. MSNホームページ経由ではなく検索エンジン広告（SEM）で獲得する同等のトラフィックを再獲得するために必要なクリック単価。ホームページ経由トラフィックが減少した場合、その「失われた」トラフィックをSEMで取り戻すコストです。

予想どおり、後者（#2）のSEMコストの方が高かったものの、直接収益以外の新規ユーザー獲得価値も考慮されており、両者はほぼ同等の評価額となりました。

実験はMSN USホームページのユーザーの5%を対象に12日間実施されました。その結果、クリック率は相対で0.38%減少し、統計的有意性（p=0.02）も確認されました。

この失われたクリック数を金銭的価値に換算したところ、予想される追加広告収益を上回ったため、ホームページへの広告追加案は見送られることとなりました。

## 2.4 Behavior-Based Search at Amazon
上記の事例はユーザーインターフェース（UI）の変更でしたが、ここではバックエンドのアルゴリズム変更に制御実験を適用した例をご紹介します。

2004年当時、著者の何人かが所属していた Amazon のデータマイニング＆パーソナライゼーション部門では、すでに「ある商品Xを購入した人は商品Yも購入している」という推薦アルゴリズム（購買ベース推薦）が確立されていました。これをさらに一般化し、「ある商品Xを閲覧した人は商品Yを購入している」「ある商品Xを閲覧した人は商品Yも閲覧している」というビューベース推薦も導入されていました。そこから新たに提案されたのが、「ある文字列Xで検索した人は商品Yを購入している」という行動ベース検索（Behavior-Based Search, BBS）です。UI上の表示には一切手を加えず、バックエンドの検索結果ランキングだけを変えることにより、検索結果の上位にこうした購買シグナルを反映させようというものでした。

たとえば、人が「24」というあいまいな検索語を入力したとき、従来の検索では『24 Italian Songs』のCDや24か月児向け衣料、24インチのタオルバーなど、さまざまな「24」を含む商品が返されてしまいます（この問題は検索語をユニークにする例外的な修飾語（例：「24 -foo」）を付けないかぎり、現在も同様に発生します）。一方 BBS アルゴリズムでは、「24」という検索を行った人が実際に購入したDVDボックスセットや関連書籍が上位に表示され、より適切な結果が得られます。アルゴリズムの強みは検索語の意味を理解せず、純粋に行動ログから学習する点ですが、その反面、検索語に含まれない商品が表示されることもあります。たとえば「Sony HD DVD Player」で検索すると、Sony製ではなく Toshiba製のHD DVDプレーヤーが上位に出てくることがあります。これは、SonyはBlu-rayプレーヤーを主に製造しており、多くのユーザーが「Sony HD DVD Player」で検索した後に Toshiba 製品を購入しているという行動シグナルによるものです。

こうした BBS の長所・短所を踏まえ、Amazon は制御実験を実施しました。その結果、2006年4月にワシントン大学 iEdge セミナーで公開されたところによると、この機能は Amazon の売上を約3%向上させ、数億ドル規模の増収をもたらしたと報告されています。

## 2.5 Other examples
これらは差分の大きさが極端な事例ですが、新しいデザインの成功を予測する難しさを如実に示しています。制御実験に関する emetrics 講演（Kohavi 2007）でも、さらに多くの事例が紹介されています。

優れた実験事例としては以下が挙げられます。
- Marketing Experiments 誌（McGlaughlin 2006）
- Design Choices Can Cripple a Website（Usborne 2005）
- Call to Action（Eisenberg & Eisenberg 2005）
- Which Sells Best（Eisenberg & Garcia 2006）

また、Forrester の Primer on A/B Testing（Chatham et al. 2004）には、ポジティブなROIを示す好例がいくつか載っています：
- Marriott は新しいオンライン予約フォームにより、追加で3,000万ドルの予約を獲得。
- Coach（ラグジュアリーアクセサリ小売業者）は、ベンダーに A/B テストで新検索エンジンの有効性を実証させることで、サイトの検索機能の効果を200%向上。
- Iomega（ディスクドライブメーカー）は、無料版ソフトウェアと製品版トライアルのどちらが好まれるか、どのメール用ランディングページが最良の転換率を生むかを実験的に検証し、キャンペーン成果を50%引き上げ。

さらに、Spool（2004）は Amtrak.com のサイトで登録成功率が4回に1回しかないことを指摘し、登録数が20%増えれば年間1,500万ドル以上の増収になると試算しています。
- InterContinental Hotels の A/B テストでは、検索結果に料金帯を追加して4,500万～6,000万ドルの追加予約を実現（Manning et al. 2006）。
- shop.com の The State of Retailing（Forrester Research 2005）では、米国137社の小売業者調査で「ユーザビリティテストとオファー・プロモーションの A/B テストを実施した小売業者の100%が、これらの手法を『効果的』または『非常に効果的』と評価」。
- 	Forrester の Web Analytics Spending Trends 2007（Burns 2006）では、ウェブ分析カテゴリで最も大幅な予算増加を見込んでいるのが A/B テストであり、主要予算増加を計画しているカテゴリは A/B テストと SEO/SEM の2つだけだったと報告しています。

# 3 Controlled experiments
「洗練された試行錯誤は、完璧な実行計画を立てることに勝る」
― IDEO 創業者 デイヴィッド・ケリー

「偉大なアイデアを得たければ、たくさんのアイデアを出しなさい」
― トーマス・エジソン

最も単純な制御実験（いわゆるA/Bテスト）では、ユーザーをランダムに二つのバリアントのいずれかに割り当てます：コントロール（A）またはトリートメント（B）です（図7参照；Mason et al. 1989; Box et al. 2005; Keppel et al. 1992）。

ここで重要なのは「ランダム」であることです。ユーザーを 「なんとなく」ばらまいてはいけません（Weiss 1997）。割り当てに影響を与える要因は一切許されません。収集された観測データに基づいて、各バリアントごとに総合評価基準（OEC: Overall Evaluation Criterion）を算出します（Roy 2001）。

たとえば、2.1節のチェックアウト事例では、OECとしてコンバージョン率、購入点数、収益、利益、顧客生涯価値の期待値、あるいはそれらを重み付けした指標などを用いることができます。得られたOECの差が統計的に有意かどうかを分析するわけです。

実験が適切に設計・実行されていれば、コントロールとトリートメントの差分以外に一貫した違いは存在しません。したがって、OECの差は必然的に割り当ての結果であり、因果関係が確立されます（Weiss 1997, p.215）。

ウェブ上での制御実験の基本的な解説書としては、Peterson (2004), Eisenberg & Eisenberg (2005), Chatham et al. (2004), Quarto-vonTivadar (2006), Miller (2006, 2007), Kaushik (2006), Peterson (2005), Tyler & Ledford (2006), Sterne (2002) などがあります。
<img width="670" height="470" alt="image" src="https://github.com/user-attachments/assets/8e740def-7124-4798-b95f-dfe673f064ae" />

概念自体は理解しやすく、基本的な考え方は多くの文献に通底していますが、ここで共有する重要な教訓はほとんど議論されることがありません。これらの教訓は、実験者が制御実験の適用範囲や限界を正しく理解し、結果を無効化してしまうようなミスを回避するのに役立ちます。

## 3.1 Terminology
御実験に関する用語は文献によって大きく異なります。以下では、本論文で使用する主要用語を定義し、一般に使われる代替用語を併記します。

Overall Evaluation Criterion（OEC）（Roy 2001）
実験の目的を定量化した評価尺度。統計学ではしばしば「応答変数（Response）」や「従属変数（Dependent Variable）」（Mason et al. 1989；Box et al. 2005）と呼ばれ、その他「アウトカム」「評価指標」「パフォーマンス指標」「適合度関数（Fitness Function）」（Quarto-vonTivadar 2006）などの呼称があります。複数の目的を持つ実験ではスコアカード方式を採用する場合もありますが（Kaplan & Norton 1996）、単一の指標（必要に応じて重み付きの組み合わせ）を選ぶことが強く推奨されます（Roy 2001, p.50）。単一指標により、複数実験間でのトレードオフを一度に決定でき、組織全体が明確な目的に沿って動けるようになります。なお、クリック数など短期的指標に偏らず、顧客生涯価値の予測や再訪率など長期的成果を予測する要因を含むOECが望ましいとされます（Ulwick 2005）。

Factor（要因）
OECに影響を与えると考えられる制御可能な実験変数。値（Value）を割り当て、これを「レベル（Level）」や「バージョン（Version）」とも呼びます。単純なA/Bテストでは要因は1つ、値はAとBの2つです。

Variant（バリアント）
要因のレベルを割り当てたユーザー体験のパターン。Control（既存版）またはTreatment（新バージョン）のいずれかです。「Treatment」と呼ぶ場合もありますが、本稿では既存版であるControlと、新たに試すTreatment variantsを区別して扱います。たとえばバグ発生時には実験を中止し、すべてのユーザーにControlを提示します。

Experimental unit（実験単位）
各バリアントで計測した指標を集計する対象単位。独立とみなされる「項目（item）」とも呼ばれます。ウェブ実験では多くの場合ユーザーですが、ユーザー日（user-day）、セッション、ページビューを単位とする場合もあります。いずれにせよ、ランダム割り当てはユーザー単位が望ましく、実験中一貫した体験を提供するためにユーザーIDをクッキーに保存して割り当てを行います（ユーザー単位以外のランダム化が適切な場合については付録参照）。

Null hypothesis（帰無仮説）
バリアント間でOECに差がなく、実験中に観測される違いはランダム揺らぎによるという仮説（H₀）。

Confidence level（信頼水準）
帰無仮説が真であるときに、棄却せずに維持する確率。

Power（検出力）
帰無仮説が偽であるときに、正しく棄却できる確率。差が実際に存在する場合に検出できる能力を表します。

A/A test（A/Aテスト）
「Null Test」とも呼ばれ（Peterson 2004）、A/Bテストと同じ要領でユーザーを2グループに割り当てるものの、両群に同一体験（両方ともControl）を提供します。
1. 検出力計算のためのデータ収集・ばらつき評価
2. 実験環境の検証（信頼水準95%ならば約5%の確率で帰無仮説が誤って棄却されるはず）

Standard deviation（標準偏差, Std-Dev）
ばらつきの指標。通常σで表されます。

Standard error（標準誤差, Std-Err）
サンプル統計量の標本分布の標準偏差（Mason et al. 1989）。独立観測値nの平均の標準誤差は σ̂/√n（σ̂は標本標準偏差）で計算されます。

## 3.2 Hypothesis testing and sample size

Treatment群がControl群と異なるかどうかを評価するには、統計的検定を行います。OECに差があると帰無仮説（OECに差はない）が棄却された場合、そのTreatmentは統計的に有意な差があると判断します。

検定の詳細は多くの統計書（Mason et al. 1989; Box et al. 2005; Keppel et al. 1992）に譲りますが、検定結果に影響を与える主な要因は以下のとおりです。
1. 信頼水準 (Confidence level)
通常95%に設定します。これは「差がないにもかかわらず差があると誤判断する確率」（第一種過誤）が5%であることを意味します。信頼水準を上げると、他の条件が同じでも検出力は低下します。
1. 検出力 (Power)
帰無仮説が偽（実際にOECに差がある）場合に、正しく棄却できる確率です。一般に80～95%程度が望まれますが、直接制御はできません。（帰無仮説を棄却せずに維持してしまう誤りが第二種過誤です。）
1. 標準誤差 (Standard error)



小さいほど検定の検出力が高まります。標準誤差を減らすには主に以下の3つの方法があります：

1. サンプルサイズを増やす
OECは大規模サンプルの平均であることが多く、平均の標準誤差はサンプルサイズの平方根に反比例します。したがって、実験を長く実施してサンプル数を増やせば標準誤差は減少し、検出力が向上します（Sect. 3.2.1 の例参照）。
1. ばらつきの小さいOEC成分を使う
標準偏差 σ が小さい指標を選ぶと、結果として標準誤差も小さくなります。たとえば、コンバージョン確率（0–100%）は購入単位数（小さな整数値）より標準偏差が小さく、購入単位数は収益（実数値）より小さい傾向があります（Sect. 3.2.1 の例参照）。
1. 不要なユーザーを除外してばらつきを抑える
たとえば、チェックアウトページの変更を評価する場合、そのページに到達したユーザーのみを対象に分析し、到達していないユーザーをOEC計算から除外すると、ノイズが減って標準誤差が下がります（Sect. 3.2.3 の例参照）。
1. 効果量 (Effect)
バリアント間のOECの差、すなわち Treatment 群の平均 – Control 群の平均 です。差が大きいほど検出が容易になるため、優れたアイデアは見逃されにくくなります。一方で、効果量が小さい場合は第二種過誤（本当は差があるのに見逃す）が起こりやすくなります。

ここで役立つ式を2つ紹介します。まずは、A/Bテスト（単因子仮説検定）で用いられるt検定の統計量です

### 3.2.1 Example: impact of lower-variability OEC on the sample size
この例では、同じ実験でも目的とする指標（OEC）を変えるだけで、必要なサンプルサイズが大きく変わることを示しています。
	•	売上（Revenue）をOECとした場合
	•	平均支出 $3.75、標準偏差 $30
	•	5% の変化を検出するには約 409,000 ユーザーが必要
	•	実験期間にして約 6 週間かかる想定
	•	コンバージョン率（購入イベント）をOECとした場合
	•	購入確率 p=0.05 のベルヌーイ試行としてモデル化
	•	標準偏差 √(p(1–p))
	•	5% の変化を検出するには約 122,000 ユーザーで十分
	•	実験期間にして約 2 週間で済む想定

つまり、ばらつきの小さい指標（ここではコンバージョン率）をOECに選ぶことで、サンプルサイズを3.3倍削減でき、実験の実行時間やコストを大幅に短縮できます。

⸻

この考え方を踏まえると、実際のABテスト設計では
1. OECの選定：短期収益ではなく、ばらつきの小さい指標（例：コンバージョン率、クリック率など）を検討する。
1. サンプルサイズ計算：検出したい効果量（ここでは5%）に合わせて、必要ユーザー数を計算する。
1. 実験期間の見積もり：自サイトのトラフィック量から、何日／週で必要サンプルに到達するかを算出し、運用計画に反映する。

といったステップを踏むのが有効です。もし別の効果量や指標での必要サンプル数を計算したい、あるいはPythonで自動計算スクリプトを組んでみたい、など具体的なご要望があればお知らせください。


### 3.2.2 Example: impact of reduced sensitivity on the sample size
感度（δ）はサンプルサイズの式で二乗して現れるため、たとえばコンバージョン率の変化を5％から20％に検出しやすくする（感度を4倍にする）と、必要なユーザー数は16分の1に減り、約7,600ユーザーで足ります。後述するように、これが実装上のバグを素早く検出できる理由です。

たとえば本来はOECの1％変化を検出するよう実験を設計していても、実装ミスでユーザーが悪い体験をしOECが20％も低下した場合、そのバグは計画期間の1/20ではなく1/400の時間で検出できます。もし実験を2週間行う予定なら、最初の1時間で重大な問題を見つけられるのです。

### 3.2.3 Example: filtering users not impacted by the change
チェックアウトプロセスに変更を加えた場合、差分を確認できずノイズだけを増やしてしまうユーザーを除外するために（3.c）、チェックアウトを「開始した」ユーザーのみを分析すべきです。仮に実験期間中のユーザーの10％がチェックアウトを開始し、そのうち50％が完了するとします。このセグメントはより均質になるため、OECのばらつき（標準偏差）が低くなります。先ほどと同じ前提を使うと、平均コンバージョン率は0.5、標準偏差も0.5となり、5％の変化を検出するにはチェックアウトを完了したユーザー6,400人で十分です

### 3.2.4 The choice of OEC must be made in advance
## 3.3 Confidence intervals for absolute and percent effect
### 3.3.1 Confidence intervals for absolute effect
### 3.3.2 Confidence intervals for percent effect

## 3.4 Effect of robots on experimental results
ロボットは推定値に著しい歪みをもたらし、実験の前提を無効化するほどの影響を与えることがあります。たとえば、A/Aテストにおいて本来有意でないはずの多くの指標が、ロボットの影響で偽陽性率が5％を大幅に超えて有意になってしまうケースを確認しています。実験を行う上では、特にユーザーIDと相互作用するタイプのロボットを除外することが重要です。あるウェブサイトでは、ロボットがサイト全体のページビューの最大半数を占めると推定されています（Kohavi et al. 2004）。多くのロボットは人間ユーザーと同様のアクセスパターンを示すため、両者を明確に区別するのは困難です。単純なロボットであれば User-Agent や IP アドレスなどの基本的な特徴でフィルタリングできますが、多くの最新ロボットは高度な手法を使って検出やフィルタを回避します（Tan and Kumar 2002）。

### 3.4.1 JavaScript versus server-side call
ロボットは推定値に著しい歪みをもたらし、実験の前提を無効化するほどの影響を与えることがあります。たとえば、A/Aテストで本来有意でないはずのメトリクスが、ロボットの影響により偽陽性率が5％を大きく超えて有意になってしまう事例も観測されています。実験を行う際には、特にユーザーIDと相互作用するタイプのロボットを除外することが重要です。あるウェブサイトでは、ロボットが全ページビューの半数近くを占めると推定されています（Kohavi et al. 2004）。多くのロボットは人間ユーザーと同様のアクセスパターンを示すため、両者を明確に区別するのは困難です。ベーシックなロボット（いわゆる“ベニン”なロボット）は User-Agent や IP アドレスなどの基本的特徴でフィルタ可能ですが、多くの最新ロボットは検出やフィルタリングを回避する高度な手法を用いています（Tan and Kumar 2002）。

### 3.4.2 Robots that reject cookies
未識別のリクエストは分析から除外することを推奨します。これにより、クッキーを拒否するロボットは実験結果に含まれなくなります。つまり、処置の割り当てやユーザー行動のデータ収集を、ユーザーのクッキーにユーザーIDが保存されている訪問者に限定すれば、これらのロボットはユーザー数にも行動データにも加えられません。

### 3.4.3 Robots that accept cookies
ロボットがクッキーを受け入れ、かつそれを削除しない場合、その影響は非常に大きくなることがあります。特に、サイト上で大量の操作を行うロボットは、Treatment 群や Control 群への混入が比較結果を著しく歪める恐れがあります。実際、あるロボットが１時間で同一ページに7,000回もクリックを行ったり、１日で3,000回以上のページビューを生成したりする例を確認しています。こうしたロボットが存在する状態で Treatment と Control を比較する仮説検定は、非常に誤解を招きやすくなります。これらのロボットは効果推定をバイアスさせるだけでなく、多くの指標の標準偏差を増大させるため、検出力も低下させます。

したがって、クッキーを削除せず、かつ単一のユーザーIDに対して大量の操作（ページビューや、onclick ハンドラによるクリックなど）を行うロボットは、積極的に除外する必要があります。一方、クッキーを受け入れない、あるいは一度あるいはごく少数の操作後にクッキーをクリアするロボットは、Treatment と Control の比較にはほとんど影響を与えません。ロボット除外は、既知のロボットの User-Agent を持つユーザーを除外リストに載せる方法と、ウェブサイトごとに異なるヒューリスティック（Kohavi 2003）を組み合わせることで実現できます。

## 3.5 Extensions for online settings


### 3.5.1 Treatment ramp-up
実験は、まずごく少数のユーザーにだけTreatmentを割り当てるところから始め、その後その割合を段階的に引き上げることができます。たとえば、最終的にA/Bテストを50%/50%で実施する場合、最初は99.9%/0.1%で始め、Treatmentの割合を0.1%→0.5%→2.5%→10%→50%と順に増やしていきます。各ステップは数時間程度実行し、その都度データを分析して、より多数のユーザーに公開する前にTreatmentに重大な問題がないかを確認します。検出力の式に現れる“割合の二乗”の効果により、少数のサンプルでも大きな異常を迅速に検知できるため、多くのユーザーが問題のあるTreatmentにさらされる前に実験を中止できるのです。

### 3.5.2 Automation

組織が明確なOECを持っていれば、自動化された探索に適した領域を最適化する実験を実行できます。たとえば、Amazon のホームページのスロットは自動的に最適化されています（Kohavi et al. 2004）。迅速な意思決定が求められる場合（例えばポータルサイトの見出し最適化など）は、誤りのコストが低いため、信頼水準を低めに設定してもかまいません。こうした最適化には、マルチアーム・バンディット・アルゴリズム（Wikipedia 2008）やホーフディング・レース（Maron and Moore 1994）などを利用できます。

### 3.5.3 Software migrations
実験はソフトウェアのマイグレーションにも役立てることができます。機能やシステムを新しいバックエンド、新しいデータベース、あるいは新しいプログラミング言語に移行する場合で、ユーザーに見える機能に変更がないことが期待されるなら、A/Bテストを実行して「バリアント間に差がない」という帰無仮説を維持することを目標にできます。

こうしたマイグレーションを複数回経験してきましたが、移行作業が完了と宣言された後でも、A/Bテストで主要な指標に有意な差異があることが判明し、移植時のバグを特定する助けとなったケースがありました。

ここでの目標は帰無仮説を維持することにあります。そのため、帰無仮説が偽である場合に確実に棄却できるだけの十分な検出力（Power）を実験に持たせることが極めて重要です。

## 3.6 Limitations
制御実験は因果関係の検証に大きな利点がありますが、その限界を理解しておくことも重要です。心理学文献で指摘されるもののうち、ウェブには当てはまらないものもありますが（Rossi et al. 2003, pp. 252–262; Weiss 1997）、私たちが実務で直面した以下の制約はぜひ押さえておいてください。
	1.	定量的指標しか得られず、「なぜ」の説明ができない
どちらのバリアントがどれだけ優れているかはわかっても、その背景にある原因までは明らかになりません。ユーザテストでは、定量データに加えて参加者のコメントなどから洞察を得られるため、ユーザビリティラボでの観察を併用して実験を補完するとよいでしょう（Nielsen 2005）。
	2.	短期効果しか測れず、長期的影響が捉えづらい
制御実験では、通常数週間の実験期間中にOECを計測します。これを「短期的視点への偏り」と批判する向きもありますが（Quarto-von Tivadar 2006; Nielsen 2005）、私たちは異論です。長期目標をOECに含める設計が肝要です。たとえば検索広告でOECを「収益」のみとすると、広告を大量に貼り付けて短期収益は上がるかもしれませんが、ユーザー体験を著しく損ねます。良いOECには、「クリックされなかった広告スペースへのペナルティ」を含めたり、「再訪率」や「離脱率」を直接測定したりする必要があります。また、ユーザーが露出後に時間差でアクションを起こす「遅延コンバージョン（latent conversions）」も考慮すべき指標です（Miller 2006; Quarto-von Tivadar 2006）。
優れたOECを設計するのは簡単ではありませんが、代替手段もありません。本質は「制御実験の限界を認識しつつ、その有用性を丸ごと捨てない」ことにあります。

3. 初期抵抗（プライマシー効果）と新規性効果
	•	ナビゲーションを変えた場合、既存ユーザーは新しい操作に慣れるまで効率が下がり、Control に有利に働く（プライマシー効果）。
	•	一方、新デザインや新機能が登場すると、ユーザーは「とにかく触ってみる」ためにあちこちクリックし、新規性バイアスを生む。
	•	このバイアスはホーソン効果とも関連づけられることがある。プライマシー効果・新規性効果いずれも、複数週間にわたる実験が必要であることを示唆する。
	•	対策例：それぞれのバリアントに初めて触れた新規ユーザーだけでOECを集計すると、両効果の影響を受けずに評価できる。

4. 機能は実装済みであること
	•	ライブ実験では、一部ユーザーにControlとは異なるTreatmentバージョンを必ず見せる必要がある。
	•	プロトタイプであったり、意図的に例外（たとえばテスト対象ブラウザの20%除外）を設けたりする場合でも、ユーザーに触らせるに足る品質で実装されていなければならない。
	•	Nielsen（2005）が指摘するように、初期段階ではペーパープロトタイピングで定性的フィードバックを得てもよいが、あくまで制御実験を補完する手法として併用すべきである。

5. 一貫性の確保
	•	ユーザーが「自分だけ別のバリアントを見ている」と気づくケースや、別端末（異なるクッキー）では異なるバリアントが表示されるケースがある。
	•	実際に気づくユーザーは少数だが、体験の一貫性を担保する仕組み（例：ユーザーIDベースのランダム化）が重要。

6. 並行実験の干渉
	•	実務ではバリアント同士の強い相互作用は稀（van Belle 2002）であり、過度に心配する必要はない。
	•	干渉の可能性を意識すれば、相互作用しそうなテストを避けられる。
	•	必要に応じてペアワイズの統計検定を行い、干渉の有無を自動的に検出することも可能。

7. リリースイベントとメディア発表
	•	新機能について大々的に発表したりメディアで取り上げたりする場合は、全ユーザーに同じバリアントを見せなければならない。


# 4 MultiVariable Testing(多変量テスト）
複数の要因を含む実験は多変量テスト（MVT: MultiVariable Test）と呼ばれることが多い（Alt and Usborne 2005）。
たとえば、MSN ホームページ上で 5 つの要因を一度にテストする場合を考えてみましょう。
図 8 には、これら各要因の Control バリアントを示す MSN ホームページのスクリーンショットを掲載しています

単一のテストでは、各要因の（主）効果だけでなく、要因間の相互作用効果も推定できます。まず、MVTと一因子ずつ行うA/Bテストの長所と短所を検討します。次に、オンラインMVTの3つのアプローチと、それぞれが潜在的なメリットをどのように活かし、制約をどのように緩和するかを論じます。

同じ要因をテストするために複数回の順次実行されるA/Bテストと比べて、単一のMVTには主に2つの利点があります。
	1.	短期間で多くの要因を同時にテストできるため、改善を加速できる。
例えば、ウェブサイトに対して5つの変更をテストしたいとします。各A/Bテストを検出力を十分に得るために4週間ずつ実行するとすれば、少なくとも5回のテストを行う必要があり、合計20週間かかることになります。
しかし、5つの要因すべてを含む単一のMVTを実行すれば、同じ検出力を維持したまま１ヶ月で完了させることができます。
	2.	要因間の相互作用を推定できる
２つの要因が結合したときの効果が、それぞれの効果の和と異なる場合を相互作用と呼びます。成果を加速するよう協調的に働く場合は「相乗的（シナジスティック）」、逆に効果を打ち消し合う場合は「拮抗的（アンタゴニスティック）」な相互作用です。

⸻

主な３つの制約
	1.	要因の組み合わせによるユーザー体験の劣化
例：オンライン小売で「商品画像を拡大表示する」「追加の詳細情報を表示する」という２つの要因は、個別では売上を改善しても、同時に適用すると“購入ボックス”がスクロール下に押し出され、結果として売上が落ちる場合があります。これは大きな拮抗的相互作用の典型例であり、企画段階で検討し、同時テストを避けるべき組み合わせです。
	2.	分析と解釈の複雑化
単一要因テストでは Treatment vs. Control の比較指標を用意すればよいのに対し、MVTでは各要因ごとに Treatment–Control の比較を行い、さらに要因間の相互作用を分析・解釈しなければなりません。情報量は増えるものの、「どの変更を採用すべきか」を判断するハードルは高くなります。
	3.	テスト開始までの準備時間
複数のA/Bテストを順次行う場合は、準備が整った要因から先にテストを開始できますが、MVTでは全要因がテスト可能な状態でないと開始できません。どれか一つの要因の準備が遅れると、実験全体の開始が後ろ倒しになります。

これらの制約はいずれも一般的に致命的ではありませんが、MVTを実施する前に認識しておくことが重要です。なお、複数要因を同時にテストする複雑さを考慮すると、最初の実験はA/Bテストから始めるのが無難でしょう。

⸻

## 4.1 Traditional MVT
このアプローチでは、製造業やその他のオフライン分野で用いられてきた実験計画を使います。典型的には、全因子計画（要因水準の全組み合わせ）の特定の部分集合である**部分因子実験計画（fractional factorial；Davies and Hay, 1950）**や、**プラケット＝バーマン計画（Plackett and Burman, 1946）**が用いられます。これらの計画は田口玄一によって普及し、**田口法（Taguchi designs）として知られることもあります。関心のある主効果や交互作用を推定できる十分な解像度（resolution）**をもつ計画を選ぶよう注意が必要です。

MSNの例では、5つの要因をテストするために、全因子計画、部分因子計画、プラケット＝バーマン計画を用いた設計例を示します。

全因子計画は、要因のすべての組み合わせを含みます。5つの要因がそれぞれ二水準なら、ユーザー群は 2^5 = 32 になります。
部分因子計画は全因子計画の一部で、ユーザー群が 2^K 個となり、各列（要因）は他の4列と直交します。ユーザー群が8や16のような分数計画は明らかに多数存在します。表1には K=3 の部分因子計画の一例を示しており、-1 がコントロール、1 がトリートメントを表します。

プラケット＝バーマン計画は、すべての要因が二水準で、ユーザー群の数が4の倍数（4, 8, 12, 16, 20, …）になるように構成できます。これらの設計でテストできる要因数は「ユーザー群数−1」です。ユーザー群数が2のべき乗である場合、プラケット＝バーマン計画は部分因子計画でもあります。

部分因子計画と同様に、与えられたユーザー群数に対して使用可能なプラケット＝バーマン計画も通常は複数存在します。

実験計画法（Design of Experiments）の統計分野では、必要なユーザー群数を最小化しつつ、主効果と交互作用を（ほとんど／まったく）交絡なく推定できる設計を見つけることが主要な研究テーマです。表1の部分因子計画は5つの主効果をすべて推定できますが、交互作用はうまく推定できません（Box et al., 2005, pp. 235–305）。多くの実験者にとってMVTを行う主目的のひとつは、テスト中の要因間の交互作用を推定することですが、この設計ではすべての交互作用が主効果や他の二因子交互作用と完全に交絡しているため、いかなる解析やデータマイニングの努力をもってしても、それぞれの交互作用を個別に推定することはできません。5要因ですべての二因子交互作用を推定したいなら、16通りのトリートメント組合せを持つ部分因子計画が必要です。表2のプラケット＝バーマン計画では、すべての二因子交互作用が主効果や他の二因子交互作用と部分的に交絡しており、これらの交互作用の推定は難しくなります。

私たちは、オンラインテストにおいては従来型のMVTより望ましいと考える2つの代替手法を推奨します。どちらを選ぶかは、交互作用の推定をどれだけ重視するかに依存します。
## 4.2 MVT by running concurrent tests

オフラインでのテストでは、たとえ実験単位数が増えなくても、処置（トリートメント）の組み合わせ数を増やすこと自体にコストがかかるため、全因子計画の部分因子が用いられます。ウェブサイトで行うテストでは、必ずしもそうする必要はありません。各要因を一因子実験として設定し、これらすべてのテストを同時並行で実行すれば、作業を簡素化しつつ、最終的に全因子計画と同等の結果を得られます。この方式では、同じユーザー集合に対して同時に開始・終了し、各実験にはユーザーを独立にランダム割当します。結果として、テストしているすべての要因について完全因子の実験が手に入ります。もちろん、全因子であれば望む任意の交互作用を推定できます。付随的な利点として、（ある要因の処置が壊滅的だった場合など）任意の要因をいつでも停止しても、他の要因には影響しない点が挙げられます。残りの要因を含む実験は影響を受けません。

一般には、処置組合せ（セル）の数が増えると検出力が下がると考えられています。各セルをコントロールのセルと個別に比較する分析を行うなら、これは当てはまるかもしれません。しかし、より伝統的なやり方で、全データを用いて主効果や交互作用を推定する分析を行う場合、検出力の低下はごく小さいか、ほとんどありません。（ある要因、または要因の組み合わせが応答の分散を増やすと、わずかな検出力低下が起こり得ます。実験誤差のプール推定を用いれば、こうした損失は最小化できます。）

サンプルサイズ（例：ユーザー数）が固定であれば、一因子をテストしていようが多因子をテストしていようが、8セルのMVTであれ全因子であれ、任意の主効果を検出する検出力は同じです（Box et al. 2005）。ただし、検出力を下げる要因が2つあります。
	1.	水準（バリアント）数を増やすこと。これは、MVTでもA/Bでも、行いたい比較の実効サンプルサイズを事実上減らします。
	2.	二水準のときに、処置割当を50%未満にすること。MVTでは特に、各処置の割当比率をコントロールと同じにすることが重要です。

## 4.3 Overlapping experiments

このアプローチは、各要因がテスト可能になった時点で一因子実験として個別に実施し、各テストを独立にランダム割り当てするというものです。前節（4.2）と異なるのは、全要因を同時に立ち上げるのではなく、各トリートメントが準備できた順に開始する点です。アジャイル開発の文脈では、デプロイ準備が整い次第テスト回数を増やして頻繁に回すことに大きな利点があります。来訪者に提示され得る要因の組み合わせで明らかなUX上の問題がない限り、これらのテストは同時進行させても構いません。

アイデアの検証速度を最大化したく、かつ交互作用に関心がない／気にしない場合に最適な手法です。大きな交互作用は（購入ボックスの例のように既知でない限り）一般に思われているほど頻繁ではありません。これは、先に述べた従来型アプローチよりもはるかに優れた代替案です。従来型では、すべての要因が準備完了するまでテストを開始できないという制約があるうえ、多くの推奨設計では**交互作用をうまく推定できない（あるいは全くできない）**ことがあります。

一方、重複（オーバーラップ）実験を用いれば、要因をより速くテストでき、任意の2要因間に十分なオーバーラップがあれば、その交互作用を推定できます。特定の2要因の交互作用に特に関心があるなら、同時期にテストする計画を立てるとよいでしょう。

まとめると、上で示した2つの代替案はいずれも、オンライン実験において従来のMVTより優れています。どちらを使うかは優先事項次第です。とにかく迅速にアイデアを試し、交互作用を気にしないなら重複実験を。交互作用の推定が重要なら、ユーザーを各テストに独立にランダム割り当てした同時実行で、実質的に全因子実験を得るのがよいでしょう。

# 5 Implementation architecture

ウェブサイトで実験を実装するには、次の3つの要素が必要です。
1. ランダム化アルゴリズム：エンドユーザーを各バリアントに写像する関数。
1. 割り当て方式（アサインメント）：ランダム化アルゴリズムの出力を用いて、各ユーザーに実際にどの体験（画面や機能）を見せるかを決定する。
1. データパス：ユーザーのサイト上での相互作用に伴って生の観測データを取得し、集計・統計処理を行い、実験結果のレポートを作成する経路。

## 5.1 Randomization algorithm
制御実験の統計的仮定は、各バリアントに無作為抽出されたエンドユーザーが割り当てられていることに依拠するため、適切なランダム化アルゴリズムを見つけることは極めて重要です。統計的に正しい実験（前述の方法論）を支えるために、ランダム化アルゴリズムは少なくとも次の3つの性質を満たさねばなりません。
1. 均等確率
（50/50分割を仮定するなら）各バリアントが表示される確率は等しく、特定のバリアントに偏らないこと。
1. 一貫した再割り当て
同一エンドユーザーが再訪しても、常に同じバリアントに割り当てられること（割り当ての一貫性）。
1. 実験間の独立性
複数の実験を同時に走らせる場合でも、実験同士に相関が生じないこと。ある実験でのバリアント割り当てが、他の実験での割り当て確率に影響しないこと。

加えて、次の2つの性質を備えていると望ましい場合があります。
1. 単調ランプアップ（Monotonic ramp-up）の支援
Treatment を見るユーザーの割合を段階的に増やしても、すでに Treatment に割り当て済みのユーザーの割り当てを変更しないこと。これにより、ユーザー体験や実験の妥当性を損なうことなく、Treatment 比率を徐々に引き上げられます。
1. 外部制御（External control）の支援
手動で特定ユーザーを特定バリアントに強制的に割り当てたり、外したりできること。これにより、実験サイトの検証が容易になります。

以降の節では、少なくとも上記最初の3つの性質を満たす手法のみを扱います。

### 5.1.1 Pseudorandom with caching
ランダム化アルゴリズムとしては、ある種のキャッシュと組み合わせれば標準的な疑似乱数生成器（PRNG）を用いることができます。良質なPRNGは、それ自体でランダム化アルゴリズムの要件1と要件3を満たします。

私たちは、要件1と要件3を満たす能力について、いくつかの一般的な乱数生成器をテストしました。連番のユーザーID100万件に対して5つの模擬実験を走らせ、カイ二乗検定で相互作用の有無を調べました。その結果、多くの言語（例：C#）に組み込まれている乱数生成器は、サーバ起動時に一度だけシードを設定する限り良好に機能することが分かりました。リクエストごとにシードを設定すると、隣接リクエストで同じシードが使われ、実験間に目立つ相関が生じる可能性があります（実際、私たちのテストではそうなりました）。特に、Eric Peterson が Visual Basic で用いた手法（Peterson 2005）は、実験間に双方向の相互作用を生じさせることが分かりました。

要件2を満たすには、アルゴリズムに状態（state）を導入する必要があります。つまり、エンドユーザーが一度サイトに来訪したら、その割り当てをキャッシュしておく必要があります。キャッシュはサーバ側（全ユーザーの割り当てを何らかのデータベースに保存）でも、クライアント側（ユーザーの割り当てをクッキーに保存）でも実現できます。
	•	データベース方式は（ハードウェア的に）高コストであり、必要な容量は実験数 × ユーザー数に比例して増えます。ただし、ユーザー割り当てDBを更新できるため、**要件5（外部制御）**を満たしやすい利点があります。
	•	クッキー方式はデータベースを要さず、必要容量もユーザーのクッキー内で実験数に線形なので、はるかに低コストです。ただし、クッキーを無効化しているユーザーには機能しません。

これらいずれの方式も、サーバ群が大規模になるとスケーリングが難しくなります。ランダム割り当てを行うサーバは、他のすべてのサーバ（バックエンド用サーバを含む）に自分の状態を伝播し、割り当ての一貫性を保たねばなりません。この種の状態伝播は高コストであり、正しく実装するのも困難です。

さらに、要件4（単調ランプアップ）はこの方法では実装が難しいため、多くのシステムでそもそも要件4を無視しています。どの方式で状態を保持するにせよ、ランプアップ後にサイトを訪れたControlユーザーの再割り当てを慎重に行う必要があります。難しいのは、全体のTreatment比率が所期の値に到達するよう、これらのユーザーにどれだけの割合でTreatmentを割り当てるかを決める点です。

### 5.1.2 Hash and partition

この方法は、乱数生成器をハッシュ関数に置き換えることで、キャッシュの必要性をなくします。乱数生成器が入力と独立に乱数を出力するのに対し、（良質な）ハッシュ関数は特定の入力の関数として乱数的に分布した数値を生成します。手順は次のとおりです。各ユーザーに一意の識別子を割り当て（データベースまたはクッキーで維持）、各実験にも一意の識別子を割り当てます。ユーザー識別子と実験識別子の組（例：連結）にハッシュ関数を適用し、ある範囲で一様分布する整数を得ます。この範囲を分割し、各バリアントに分割（パーティション）を対応づけます。ユーザーの一意識別子は、任意の数の実験に再利用できます。

この方法の成否はハッシュ関数の選択に非常に敏感です。もしハッシュ関数に「ファネル（隣接するキーが同一のハッシュコードに写像される事象）」があると、性質1（均一分布）が破られます。さらに、キーをわずかに変えるとハッシュ値の変化が予測可能になる特性があると、実験間に相関が生じ得ます。実務でこの手法に耐えるほど健全なハッシュ関数は多くありません。

私たちは複数の一般的なハッシュ関数を用い、疑似乱数生成器の場合と同様の方法でこの手法を検証しました。定義上、どのハッシュ関数も性質2（同一ユーザーの一貫した再割り当て）は満たしますが、性質1と性質3（実験間の独立性）を満たすのは難しいことが分かりました。連番ユーザーID100万件に対して5つの模擬実験を走らせ、ランダム化アルゴリズムの性質1と3の違反をカイ二乗検定で確認したところ、実験間に相関を生じさせなかったのはMD5（暗号学的ハッシュ）だけでした。SHA-256（同じく暗号学的ハッシュ）も近い結果でしたが、5次の相互作用を仮定すると相関が発生しました。.NET に組み込まれた文字列ハッシュを含むその他のハッシュ関数は、2次の相互作用のテストすら通過できませんでした。

このハッシュ＆パーティション方式の実行時性能は、使用するハッシュ関数の性能に制約されます。MD5 のような暗号学的ハッシュは計算コストが高く、部分的なキャッシュで性能を高めようとする工夫は概してうまくいきません。例えば、実験名とユーザーIDをそれぞれ別々にハッシュ化しておき、割り当て時にXORで合成する方式を試みたシステムがありました。狙いは、実験名のハッシュを実験側に、ユーザーIDのハッシュをユーザーのクッキーにキャッシュしておき、割り当て時には最終的な XOR のみ実行することで MD5 の計算を避けることでした。しかしこの方法は実験間に深刻な相関を生みます。例えば、2つの実験がいずれも 50/50 の2バリアントだとすると、両実験の「実験名ハッシュの最上位ビット」が一致すればユーザーは常に同じ割り当てを受け、異なれば常に逆の割り当てになります。いずれにせよ性質3に反し、両実験の結果は交絡します。

性質5（外部制御）を生のハッシュ＆パーティション方式で満たすのは非常に困難です。最も単純な解は、ハッシュ＆パーティションと小規模なデータベースあるいは限定的なクッキー利用を組み合わせるハイブリッド方式です。外部制御の対象ユーザー集合（例：テストチームが指定するユーザー）は通常ごく少数なので、このハイブリッド方式なら、疑似乱数＋キャッシュ法が抱える欠点の全面的な影響は受けにくくなります。

## 5.2 Assignment method
割り当て方式（assignment method）とは、実験中のウェブサイトがエンドユーザーごとに異なるコードパスを実行できるようにするソフトウェア要素のことです。良い割り当て方式は、画面に見えるサイトコンテンツからバックエンドのアルゴリズムまで、あらゆる部分を切り替えて操作できます。割り当て方式の実装には複数の手法があり、本節の残りでは一般的な方式を比較し、その活用におけるベストプラクティスを推奨します。

### 5.2.1 Traffic splitting
<img width="739" height="225" alt="image" src="https://github.com/user-attachments/assets/6f51fda7-c7f8-465d-b6e2-8722f09614a5" />


Traffic splitting（トラフィック分割）とは、実験の各バリアントを別々の論理的サーバ群（fleet）上に実装するタイプの割り当て方式の総称です。これは、物理的に異なるサーバ、仮想的に異なるサーバ、あるいは同一マシン上の異なるポートでも構いません。サイトはロードバランサまたはプロキシサーバを用いてバリアント間でトラフィックを分割し、ランダム化アルゴリズムはこの層に組み込む必要があります。
Traffic splitting の利点は**非侵襲的（non-intrusive）**である点で、既存コードを変更せずに実験を実装できます。
しかし、この手法には重要な欠点があります。
1. 小規模な機能変更でも負担が大きい。 変更規模に関わらずアプリ全体を複製する必要があるため、スモールチェンジには不釣り合い。
1. 並列フリートの構築・設定コストが高い。 実験停止時に備え、Control フリートはトラフィック100%を受けられる容量が必要。Treatment フリートは小さくできるが、その規模が割当可能な最大割合の上限になる。
1. 複数実験の同時実行が急速に複雑化。 すべての実験にわたるバリアント組合せごとにパーティションを用意する必要があり、組合せ数に応じて（同時実験数に対して指数関数的に）増える可能性がある。
1. フリート間差異が結果を交絡させるリスク。 理想的には各フリートのハードウェア／ネットワーク構成を同一にし、A/A テストでフリート起因の効果がないことを確認すべき。

見た目には IT／開発の手間を抑えられて安上がりに見えるものの、実際にはコストの高い実装方法になりがちです。
この方法は、新しいサイト基盤への移行、新レンダリングエンジン導入、サイトの全面的アップグレードなど、コードが大きく異なる変更をテストする用途に推奨されます。

### 5.2.2 Page rewriting

<img width="759" height="170" alt="image" src="https://github.com/user-attachments/assets/b786940b-571e-4952-9c40-186e776a6e86" />

ページ書き換え（Page rewriting）は、エンドユーザーにHTMLを返す前にプロキシサーバがHTML内容を改変する割り当て方式です。具体的には、エンドユーザーのブラウザがプロキシにリクエストを送り、プロキシは一部データを記録したのち実験対象サイトへ転送します。サイトから返ってきたHTMLレスポンスはブラウザへ返る途中で再びプロキシを通過します。ここでプロキシはランダム化アルゴリズムを適用し、1つ以上の実験に対するバリアントを選択し、選択結果に従ってHTMLを変更します（例：正規表現やXPathによる置換ルールの適用）。その後、修正済みHTMLがエンドユーザーのブラウザに送信されます。

この方式に基づく商用ソリューションは少なくとも1社（SiteSpect 2008）が提供しています。トラフィック分割と同様、非侵襲的であり、既存コードの変更を要しません。ただし、次の欠点があります。
1. ページ表示時間への影響
プロキシによる書き換え処理時間と、プロキシとWebサーバ間のネットワーク遅延の双方がレンダリング時間を悪化させます。
1.	大規模サイトではハードウェア要件が大きい
プロキシはサイトへの全トラフィックを処理し、かつ**単一点障害（SPOF）**になり得るため、スケーラビリティと可用性を確保するには多数のサーバが必要になりがちです。
1.	バリアントの開発・テストが難しくエラーを招きやすい
各バリアントは「HTMLそのもの」ではなく「HTMLを書き換える規則」として表現する必要があり、他方式に比べて取り扱いが難しくなります。
1.バックエンドアルゴリズムの実験が難しい
割り当て判断がサイト側でページがレンダリングされた後に行われるため、バックエンド変更の評価には不向きです。
1. 暗号化トラフィック（https）での実験は高コスト
プロキシがコンテンツを復号→改変→再暗号化する必要があり、計算資源を多く消費します。特に、サイトで最も重要な領域（例：チェックアウト）は一般に暗号化されているため、大きな課題になります。

総じて、ページ書き換えはフロントエンドのコンテンツを対象とする実験を、IT／開発の関与を最小化しつつ安価に実施する方法になり得ますが、バックエンド変更やプラットフォーム移行のテストには適しません。


### 5.2.3 Client-side assignment

<img width="733" height="205" alt="image" src="https://github.com/user-attachments/assets/bd2e56a2-fcc4-4367-b76e-a529f326b4a5" />
クライアント側でのページ改変は、サードパーティの実験プラットフォームで最も一般的な割り当て方式です。Google Website Optimizer（2008）、Omniture の Offermatica（Omniture 2008）、Interwoven の Optimost（2008）、Widemile（2008）、Verster（2008）など多くの製品が対応しています。

これらすべての製品は、サーバ側で意思決定を行うことなく実験を実行できます。開発者は、レンダリング時にエンドユーザーのブラウザから割り当てサービスを呼び出すよう指示する JavaScript をページに挿入して実験を実装します。サービス呼び出しは、そのユーザーに適したバリアントを返し、続いて JavaScript のコールバックをトリガーして、DOM を変更するなどして表示中のページを動的に書き換えます。変更はページのどの部分よりも先に行われなければならないため、サービス呼び出しの遅延はページ全体の描画時間に加算されます。各バリアントのコンテンツは、ページに巧妙に埋め込むことも、割り当てサービスから配信することもできます。この方式は侵襲的ではあるものの、実装は非常に容易で、開発者はページに短い JavaScript スニペットを追加するだけで済みます。ただし、いくつかの重要な制約があります。
1. 初期レスポンス後に実行されるため遅延を招く
クライアント側の割り当てロジックは最初のページ配信後に実行されるため、割り当てサービスの過負荷、回線の遅さ、ユーザーと割り当てサーバの地理的距離などにより、エンドユーザー体験が遅延します。
1. 動的コンテンツが多い複雑なサイトでは適用が難しい
複雑なコンテンツが、ページを書き換える JavaScript と相互作用し、予期せぬ不具合を生む可能性があります。
1. 実験対象であることがユーザーに露見しやすい
ブラウザの「ページのソースを表示」で、ページが実験の対象であることが分かったり、実装によっては各バリアントのコンテンツを抽出できてしまう場合があります。

注意：この方式の一部実装では、クッキーによりユーザーが Control だと分かっている場合にサービス呼び出しを省略して描画時間を最適化しようとします。しかしこれは誤り（使用すべきでない）で、描画遅延とバリアント割り当てが相関してしまい、実験に交絡因子を導入します。

この方式は、主として静的なフロントエンドコンテンツに対する実験に最適です。

### 5.2.4 Server-side assignment
<img width="756" height="187" alt="image" src="https://github.com/user-attachments/assets/87e3659e-d4ad-49b9-ab18-caf0faf0acbe" />

サーバーサイド割り当てとは、サイトのサーバー側に埋め込んだコードを用いて、バリアントごとに異なるユーザー体験を生成する一連の手法を指します。コードは、バリアント間でサイトのロジックが分岐する箇所に配置された API 呼び出しの形をとります。API はランダム化アルゴリズムを起動し、現在のユーザーに表示すべきバリアントの識別子を返します。呼び出し元のコードはこの情報を使って、各バリアントに応じた別のコードパスへ分岐します。API 呼び出しはサーバー側のどこにでも置けます――フロントエンドのレンダリングコード、バックエンドのアルゴリズムコード、さらにはサイトのコンテンツ管理システム（CMS）内でも構いません。複雑な実験では、コードの異なる場所に複数の API 呼び出しを挿入する場合もあります。API はローカル関数として実装することもできますが、一般には大規模なサーバーフリート全体で割り当てロジックの一貫性を保つため、外部サービスを用います。サーバーサイド割り当ては非常に“侵襲的”で、実験対象アプリケーションのコードに大きな変更を要します。それでも次の3つの明確な利点があります。
1. 極めて汎用的：ページのあらゆる側面を、コードを変更するだけで実験できます。
1. 論理的に最適な位置で分岐：変更に関する意思決定が行われる箇所に実験コードを置けます。とりわけ、フロントエンドに手を入れずに検索やパーソナライゼーションといったバックエンド機能を実験できます。
1. ユーザーに透明：エンドユーザーからは実験の存在が分からず、遅延も最小限です。

同時に、“侵襲的”であるがゆえの欠点もあります。
1. 初期実装コストが高い：サイトの複雑さによっては、必要なサーバー側コードの変更が難しいことがあります。
1. 実験ごとに深いコード変更が必要：ページロジックの深部を変更するため、実験の実装自体がリスクを伴います。特に、コードが多数のページやサービスに分散している複雑な機能ではリスクが大きくなります。
1. 後片付けの負担：実装のバリエーションによっては、実験完了時にコードを手作業で巻き戻す必要があります。具体的には、負けたトリートメントを実装するコードパスや、ユーザーの割り当てに応じて分岐する条件ロジックを削除しなければなりません。単一ページの簡単な実験なら容易ですが、API 呼び出しがコード全体に散在していると面倒で、こうした変更はすべて追加のリスクを生みます。負けたトリートメントを残すとコードベースが乱雑になり、削除すると本番コードを変更するリスクが増します。

サーバーサイド割り当ては CMS と統合することで、この方式で実験を回すコストを大幅に下げられます。統合後は、実験はコードではなくメタデータの変更で構成されます。メタデータは、編集可能な設定ファイルから、GUI で管理するリレーショナルデータベースまで、さまざまな形で表現できます。以下では Amazon.com の実システムの例でこの方法を示します。

Amazon のホームページは、スロット（slot）と呼ばれる個々のユニットからページを組み立てる CMS の上に構築されています（Kohavi et al., 2004）。このシステムはレンダリング時にページのメタデータを参照し、ページの組み立て方法を決定します。非技術職のコンテンツエディターは、GUI を通じてこのページメタデータを編集し、各スロットに表示するコンテンツをスケジューリングします。コンテンツには、広告、商品画像、リンク付きのテキストスニペット、動的コンテンツ（例えばパーソナライズ推薦）を表示するウィジェットなど、あらゆるものを含められます。典型的な実験は、異なる場所にさまざまなコンテンツを配置してみることです。たとえば「推薦を左に置いた方が右に置くよりクリック率が高いか？」といった問いです。こうした実験を可能にするため、CMS を拡張し、特定の実験にひもづけてコンテンツをスケジューリングできるようにします。ページリクエストが到来すると、システムはスケジュールされた各実験の割り当てロジックを実行し、その結果をページコンテキストに保存し、ページ組み立て機構がそれに応じて動作できるようにします。CMS 自体の改修は一度だけで済み、その後はユーザーインターフェースからページメタデータを変更するだけで、実験の設計・実装・終了が可能になります。

### 5.2.5 Summary
以下の表は、上述のすべての割り当て方式について、その相対的な長所と短所を要約したものです。


手法カテゴリ	侵襲性？	初回実験の実装コスト	以降の実験の実装コスト	ハードウェアコスト	柔軟性	レンダリング時間への影響
トラフィック分割 (Traffic splitting)	いいえ	中〜高	中〜高	高	高	低
ページ書き換え (Page rewriting)	いいえ	中	中	中〜高	中	高
クライアント側割り当て (Client-side assignment)	はい（中程度）	中	中	低	低	高
サーバー側割り当て (Server-side assignment)	はい（高い）	高	中〜低	低	非常に高い	非常に低い
### 5.3 Data path
実験バリアント間で指標（メトリクス）を比較するために、まずサイトは実験期間中に訪れたすべてのエンドユーザーの処置（トリートメント）割り当てを記録する必要があります。続いて、ページビュー、クリック、収益、レンダリング時間、顧客フィードバックの選択といった生データを収集します。生データの各行には、そのページリクエストでユーザーが見た各実験のバリアントIDを注記します。

その後、システムは生データをメトリクス（数値要約）へと変換し、実験のバリアント間で結果を比較できるようにします。メトリクスは、総ページビューのような単純な集計から、顧客満足度や検索関連性のような推定指標まで幅があります。メトリクスを計算する際には、基本的な変換を適用しつつ、観測値を実験・バリアント・分析したい他のディメンション（例：デモグラフィック、ユーザーエージェント）でグルーピングして集計します。必要に応じて、さらに変換を施してより複雑な指標を作ることもあります。

こうして、ディメンション × 実験 ×（特に）バリアントで分解されたメトリクス値の表を作成します。あとは、バリアント間でメトリクスを比較し、適切な統計検定を用いて統計的有意性を判定します。

なお、基本的な分析手法は OLAP によく似ていますが、ウェブサイト実験には固有のデータ上の課題が生じます。
### 5.3.1 Event-triggered filtering

大規模サイトのウェブトラフィックから収集されるデータは、通常きわめて大きなばらつきを含むため、小規模な機能の効果を十分な検出力で見つけられるように実験を行うのは難しくなります。
このばらつきを抑える重要な方法の一つは、実験の影響を受けたユーザーのみに分析対象を限定することです（3.2.3節参照）。さらに、ユーザー行動のうち実験の影響を受けた部分にまで分析を絞り込むこともできます。私たちはこのようなデータの限定を**イベントトリガー型フィルタリング（event-triggered filtering）**と呼びます。

イベントトリガー型フィルタリングは、各ユーザーが実験の影響を受けたコンテンツを初めて見た時刻を追跡することで実装します。このデータは、（i）ユーザーが実験コンテンツを閲覧した瞬間にイベントを記録する直接的な方法、または（ii）ページビューや既存の生データストリームの他の部分から実験コンテンツを特定する間接的な方法で収集できます。
また、このフィルタリングを割り当て方式（assignment method）に直接統合することも可能です。

### 5.3.2 Raw data collection

# 6 Lessons learned
「理論と実践のギャップは、理論上で想定されるギャップよりも、実際の現場でのギャップのほうがはるかに大きい」
― ヤン・L.A.・ファン・デ・スネプスホイト

## 6.1 Analysis
「地獄への道は善意で舗装され、その道はずさんな分析で散らかっている。」
— 匿名

### 6.1.1 Mine the data
制御実験は、OEC の差が統計的に有意かどうかという単なる1ビットの情報以上を提供します。通常、機械学習やデータマイニングで分析できるリッチなデータが収集されます。たとえば、全体では有意差が見られなかったものの、特定のブラウザバージョンのユーザー集団では Treatment の成績が有意に悪化していた、というケースがあります。調べると、JavaScript を用いた当該 Treatment の機能がそのブラウザバージョンでバグを起こし、ユーザーが離脱していたのです。その集団を分析から除外するとポジティブな結果が現れ、バグ修正後に機能を再テストしたところ、実際に良い結果が得られました。

### 6.1.2 Speed matters
パフォーマンス低下によって、Treatment（新仕様）がユーザー体験を悪化させる場合があります。
Linden（2006b, p.15）は、Amazon の実験で表示が100ミリ秒遅くなるごとに売上が1%減少したこと、また Google の特定の実験では検索結果の表示を500ミリ秒遅らせると収益が20%減少したこと（Web 2.0 での Marissa Mayer の講演に基づく）を報告しています。Microsoft Live Search の最近の実験（Kohavi 2007, p.12）でも、検索結果ページを1秒遅くするとユーザー当たりのクエリが1%減、広告クリックが1.5%減、2秒遅くするとそれぞれ2.5%減と4.4%減に悪化することが示されました。

時間がOECに直接含まれていない場合でも、新機能が負けている原因が動作の遅さではないか、必ず確認してください。

### 6.1.3 Test one factor at a time (or not)

いくつかの著者（Peterson 2004, p.76; Eisenberg 2005）は**「一度に1要因だけをテストせよ」と推奨していますが、この助言を文字どおり狭く解釈すると制約が強すぎ、組織が小さな漸進的改善に偏りがちになります。逆に、フラクショナル・ファクトリアル設計や田口法を喧伝する企業もあり、不要な複雑さを持ち込むことがあります。確かに、因子計画は要因の同時最適化を可能にし、理論上は優れています（Mason et al. 1989; Box et al. 2005）。しかし、私たちがウェブ上で実験を多数運用した経験では、相互作用は人々が想定するほど頻繁ではなく（van Belle 2002）、この点を認識していれば干渉しそうな実験を並行で走らせない**よう回避できます。

したがって、私たちの推奨は次のとおりです。
- 洞察獲得や分離可能な漸進的変更には、単一要因実験を行う。
- 思い切った賭けや大きく異なるデザインも試す。例：2人のデザイナーに新機能のまったく異なる2案を作らせ、A対Bで競わせる。その後、勝ったバージョンを摂動（微調整）してさらに改善する。バックエンドのアルゴリズムなら、まったく別のアルゴリズム（例：新しい推薦アルゴリズム）を試すのはさらに容易。データマイニングで新アルゴリズムが有意に強い領域を特定でき、有益な洞察につながる。
- 強い相互作用が疑われる複数要因を扱う場合は、相互作用推定に適した全因子／部分因子設計を用いる。各要因の水準数を絞り、Treatment と Control の割当比率を同じにする。これにより、効果検出のための検出力を最大化できる。

## 6.2 Trust and execution
「神を信じる――ほかの者は現金払いだ」
— ジーン・シェパード

### 6.2.1 Run continuous A/A tests

A/A テスト（§3.1 参照）を実行し、次を検証すること。
1. ユーザーが計画した割合どおりに分割されているか？
1. 収集データはシステム・オブ・レコード（SoR／正本データ系）と一致しているか？
1. 95% のケースで有意差が出ていないこと（＝第一種過誤が約 5% に収まっているか）。

また、他の実験と並行してA/A テストを継続的に走らせること。

### 6.2.2 Automate ramp-up and abort

第3.3節で述べたとおり、実験では Treatment（群）に割り当てるユーザーの割合を段階的に増やすことを推奨します。実験データをほぼリアルタイムで分析できる実験システムであれば、Control と比べて有意に劣後している Treatment を自動停止（auto-abort）できます。自動停止は、当該 Treatment に割り当てるユーザー割合を0% に落とすだけです。

この仕組みにより、多数のユーザーを重大な不具合にさらすリスクをシステムが自動で下げてくれるため、組織は思い切った賭けができ、より速くイノベーションを進められます。ランプアップはオンライン環境では容易ですが、オフライン研究では難しいのが実情です。これらの実践的アイデアは文献ではほとんど言及されていませんが、極めて有用です。

### 6.2.3 Determine the minimum sample size
統計的検出力、検出したい効果（効果量）、そして A/A テストで OEC のばらつきを見積もってください。そのデータに基づき、必要な最小サンプルサイズと、ひいては自サイトでの実験期間を算出できます。よくある誤りは、**検出力不足（underpowered）**の実験を走らせてしまうことです。OEC のばらつきを下げるために、3.2 節のポイント 3 で述べた手法の活用を検討してください。さらに、一部の指標にはパワー特性が悪く、実験を長く実施するほど検出力がむしろ低下するものがあることも認識しておきましょう。こうした指標では、1 日あたり十分なユーザー数をテストに取り込むこと、および Treatment と Control の群サイズを等しく保つことが重要です。

### 6.2.4 Assign 50% of users to treatment
新米の実験者にありがちな慣行の一つは、新バリアントをごく少数のユーザーにだけ見せることです。エラーがあっても影響範囲を小さくできる、という発想ですが、そのためにこそ段階的ランプアップを推奨しています。

一方で、検出力を最大化し実験期間を最小化するには、A/B テストでは各バリアントを50%/50%で見せることを勧めます。その他の条件が同じなら、Treatment に割り当てる比率を p としたとき、50/50 実行に対する必要実行時間の倍率の近似は

\text{倍率} \approx \frac{1}{4\,p(1-p)}

となります。たとえば 99%/1% で実施すると、

\frac{1}{4 \times 0.01 \times 0.99} \approx 25

となり、50%/50% で走らせる場合より約25倍長く実行しなければ同じ検出力になりません。

### 6.2.5 Beware of day of week effects
サイトの来訪者が多く、数時間〜1日で実験を終えられそうに見えても、最低でも1〜2週間は実施し、その後も週単位で延長することを強く推奨します。これは曜日による効果を分析するためです。多くのサイトでは、週末の訪問者は平日とは異なるセグメントを構成しており、彼らを分けて分析すると有益な示唆が得られることがあります。この教訓は、祝日や季節といった他の時間的イベントや、地域差にも一般化できます。米国でうまくいく施策が、フランス、ドイツ、日本でも同様に機能するとは限りません。

6.2.3、6.2.4、6.2.5 を総合すると、たとえば検出力計算の結果、50%/50% のA/Bテストなら最短5日でよいと示された場合でも、曜日効果を避け、最低限よりも検出力を高めるために1週間の実施を勧めます。
一方、95%/5% で実施するなら、必要期間は約5倍（5〜25倍レンジの下限寄り）に増えるため、4週間の実施を推奨します。
99%/1% では25倍超となり、125日以上が必要になりますが、これは信頼できる結果を得るには長すぎる期間です。数週間の実験では二次的影響にとどまる要因（例：クッキーチャーン）が、データを汚染し始める恐れがあるためです。

### 6.3 Culture and business
「その人の給料が“それを理解しないこと”にかかっているとき、その人にそれを理解させるのは難しい。」
― アプトン・シンクレア

### 6.3.1 Agree on the OEC upfront
制御実験の強みの一つは、新機能のビジネス価値を客観的に測定できることにあります。ただし、その目的を最大限に果たすには、実験を実施する前に関係者が評価方法に合意しておくことが重要です。

これは一見当たり前に聞こえますが、実際にはあまり徹底されません。多くのオンライン機能の評価は、しばしば相反する複数の目標に左右されるからです。OEC（総合評価基準）は複合指標として設計でき、実験で観測される複数の目的を単一の指標へ統合します。OECを策定する過程では、組織は各入力の価値を評価し、その相対的重要度を決める必要があります。

有効な手法として、ユーザーやその行動のライフタイムバリューを見積もることが挙げられます。たとえば、新規ユーザーの検索は、既存ユーザーの追加検索より高い価値を持つかもしれません。実験の実施自体に単一指標は必須ではありませんが、このような事前の合意形成と設計の手間をかけることで、組織の足並みがそろい、目標が明確になります。

### 6.3.2 Beware of launching features that “do not hurt” users

実験でバリアント間に統計的に有意な差が出なかった場合、それは本当に差がないのか、あるいは変化を検出するだけの検出力（パワー）が不足していたのかもしれません。
「有意差なし」という結果を前に、「害はないから」と変更をそのままリリースしてしまう判断がなされることがあります。しかし実際には負の効果があるのに、検出力不足のせいで見逃している可能性もあります。

### 6.3.3 Weigh the feature maintenance costs
実験でバリアント間に統計的に有意な差が出たとしても、保守コストを考えると新バリアントのリリースが正当化されない場合があります。OECのわずかな改善が、その機能を維持・運用するコストに見合わないことがあるのです。


### 6.3.4 Change to a data-driven culture

# 7 Summary
「ほとんどあらゆる疑問は、テストキャンペーンによって安価に、迅速に、そして最終的に解決できる。そしてそれが答えを出す方法なのだ――会議室で議論するのではなく。最後の審判の場へ行け――あなたの商品を買ってくれる人々のもとへ。」
― クロード・ホプキンス『Scientific Advertising』, 1923年

「ウェブベースのアプリケーションにおいて、容易に実験できる能力は極めて重要だ。オンラインの世界は決して静止しない。新しいユーザー、新しい製品、新しい技術が絶えず流入している。何がうまく機能し、何が機能しないかを迅速に見定めることが、生き残りか絶滅かを分けるのだ。」
― ハル・ヴァリアン, 2007年

従来の知識発見（Knowledge Discovery）やデータマイニングは洞察を提供しますが、発見されるパターンは相関関係にとどまり、“リーク”（情報漏洩）によって生じたものと、有用かつ実行可能なパターンとを切り分けるのが難しいという課題があります（Kohavi et al. 2004）。一方、制御実験ではランダム割り当てによって交絡変数をすべてのバリアントに均等に分散させるため、異なるバリアント間で行った変更と、Overall Evaluation Criterion（OEC）を含む評価指標との因果関係を確立できます（Keppel et al. 1992）。このような環境下でデータマイニング技術を併用すると、たとえば制御実験で導入した機能の恩恵を受けるユーザーセグメントを特定するといった、機能改善やパーソナライゼーションの好循環を生む極めて有益な洞察を得ることが可能になります。

基本的な制御実験の考え方自体は理解しやすいものの、ウェブ向けに体系的にまとめられた解説はこれまで存在しませんでした。本稿では、汎用的なアーキテクチャ、段階的導入と自動中断、実践上のランダム化・ハッシュ化の問題点、そしてOECにまつわる組織的課題など、新たに得られた教訓や洞察を随所で共有しています。

今日の製品におけるソフトウェア機能は、第二次世界大戦前の医療における処方と同様に、専門家の判断に基づいて決められることが一般的でした。しかし、オンラインで顧客行動を直接観察できる現代では、もっと良いやり方が可能です。Marks 著『The Progress of Experiment: Science and Therapeutic Reform in the United States, 1900–1990』（2000年、p. 3）には、医療知識の発展における計画実験の重要性が次のように述べられています：

「世紀の後半の改革者たちは、経験豊富な臨床医の判断への信頼を捨て去った。その代わりに、無作為化二重盲検対照試験という、感情を排した科学的厳格さの基準を提示した。」

多くの組織では、強い意見を持つマネージャーがデータを持たないまま意思決定を行います。そこで私たちは“HiPPO（Highest Paid Person’s Opinion）＝最高報酬者の意見”という言葉を使い、最終的にモノを言うのはユーザーの評価であることを常に意識するようにしています。一部の著者は実験手法を「イノベーションへの新たな必須条件（New Imperative for Innovation）」（Thomke 2001）と呼び、「新しい技術のおかげで、これまで以上に迅速かつ安価に複雑な実験が行えるようになっている」と指摘しています。私たちも同感であり、顧客体験こそが最終的に重要である以上、常に実験を通じてユーザーの声に耳を傾けることで、企業はイノベーションを加速できると考えています。
