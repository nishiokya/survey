# 論文情報・リンク
論文リンク： https://ai.stanford.edu/~ronnyk/2009controlledExperimentsOnTheWebSurvey.pdf
公開日時： Published online: 30 July 2008; Journal issue: February 2009 ￼ ￼
実装コード： 内部実装プラットフォーム「ExP」（Microsoft内部・非公開）を紹介 ￼
Publication : Data Mining and Knowledge Discovery, Vol. 18, No. 1, pp. 140–181 (2009) ￼

# Abstract

制御実験（ランダム化実験、A/Bテストおよびその拡張、スプリットテスト、Control/Treatmentテスト、多変量テスト（MVT）、並行フライトとも呼ばれる）は、変更とユーザーの観察可能な行動への因果関係を確立するための、最も理にかなった科学的デザインを体現しています。本稿では、エンドユーザーの反応を活かして機能開発を導くオンライン実験の実践的ガイドを提供します。私たちの経験から、開発チームが「Highest Paid Person’s Opinion（HiPPO）」ではなく顧客の声に耳を傾けることで、大きな学びと投資収益率（ROI）が得られることが明らかになっています。

本稿では、驚くべき結果をもたらしたいくつかの制御実験事例を紹介するとともに、実験を実行する上で必要な重要要素と、その技術的・組織的な制約について論じます。特に、統計的検出力、サンプルサイズ、分散削減のための手法といった、実験設計における要所に焦点を当てています。

また、実験システムの一般的なアーキテクチャを説明し、それぞれの利点・欠点を分析します。さらに、実際には単純とは言えないランダム化およびハッシュ化技術についても評価を行います。

制御実験は通常、大量のデータを生み出しますが、これをデータマイニング技術で分析することで、結果に影響を与える要因を深く理解し、新たな仮説を生み出し、改善の好循環を生み出せます。明確な評価基準のもとで制御実験を取り入れる組織は、自動最適化やリアルタイム分析を通じてシステムを継続的に進化させることが可能です。

私たちは複数のシステムや組織での豊富な実践経験に基づき、信頼性の高い制御実験を実施するための重要な教訓を本稿で共有します。


# 1 Introduction
「千の専門家の意見に勝るは、一つの正確な測定である」
― アドミラル・グレース・ホッパー
1700年代、ある英国海軍の船長は、地中海方面の艦隊に配属された水兵が壊血病（スコルブ）の症状をほとんど示さないことに気づきました。彼らの配給食には柑橘類が含まれていたのです。そこで船長は乗組員を二つのグループに分け、一方（Treatment 群）にはライムを与え、もう一方（Control 群）には従来通りの食事を続けさせる実験を行いました。Treatment 群では不満の声も上がりましたが、実験は成功し、ライムの摂取が壊血病の予防に効果的であることが示されました。船長自身は壊血病がビタミンC欠乏によるものであることや、ライムに豊富にビタミンCが含まれていることを理解していませんでしたが、この介入は確実に功を奏しました。やがて英国水兵には定期的に柑橘類を摂取することが義務づけられ、その習慣が「ライミーズ（limeys）」という愛称を生むに至りました（Rossi et al. 2003; Marks 2000）。

それから約300年後、Amazon のグレッグ・リンデンは、ショッピングカートに入れた商品の履歴に応じてパーソナライズされた推薦を表示するプロトタイプを開発しました（Linden 2006a, b）。商品を一つ追加すると推薦が現れ、別の商品を追加すると別の推薦が表示される仕組みです。このアイデアは一見有望でしたが、あるマーケティング上級副社長は「チェックアウトの妨げになる」と断固反対し、リンデンは「これ以上手をつけるな」と命じられました。しかし、彼はそれでも制御実験を実施し、その機能は圧倒的な差で勝利を収め、「未導入のままでいることが Amazon にかなりの損失を与えている」ことが明らかになりました。急遽ショッピングカート推薦機能が本番導入され、その後、多くのサイトが同様の仕組みを採用しています。

本論文の著者たちは、Amazon、Microsoft、DuPont、NASA などで数多くの実験に携わってきました。特に Amazon では「直感よりデータを重視する」という文化と、実験を容易に行えるシステムが整備されており、迅速かつ効果的なイノベーションを可能にしています。Microsoft においても複数の制御実験システムが運用されており、本論文ではそれぞれのアーキテクチャの利点と欠点を解説します。本稿の一貫したテーマは、制御実験が優れた投資収益率（ROI）を生み出し、適切なインフラストラクチャを構築することでイノベーションのスピードを加速できるという点です。Stefan Thomke の著書『Experimentation Matters』（Thomke 2003）のタイトルは、その趣旨を端的に示しています。

Webは、制御実験（ランダム化実験：単因子設計または因子設計）、A/Bテスト（およびその一般化）、スプリットテスト、Control/Treatmentテスト、並行フライトといった手法を用いて、アイデアを迅速に評価するための前例のない機会を提供します。これらの実験の最もシンプルな形では、実際のユーザーをランダムに二つのバリアントのいずれかに割り当てます。(i) Control（通常は「既存」バージョン）、(ii) Treatment（評価対象となる新バージョン）です。実行時パフォーマンスから、暗黙的・明示的ユーザー行動、アンケートデータまで、多様な指標を収集し、統計検定を適用して、両バリアント間に有意差があるかどうかを判断します。これにより、「バージョン間に差はない」という帰無仮説を維持するか棄却するかを決定できます。さらに、OLAPなどの手動分析や機械学習・データマイニング技術によってユーザーセグメントを掘り下げることで、どのサブポピュレーションにおいて差が顕著かを明らかにし、アイデア理解を深め、次のステップへ進む助けとします。

制御実験は、アイデアを確実に評価するための方法論を提供します。事後分析や中断時系列分析（準実験法）などとは異なり、本手法は因果関係を検証するための実験デザインです（Keppel et al. 1992, pp. 5–6）。多くの組織には多彩なアイデアがありますが、そのROI（投資収益率）が不明瞭だったり、評価コストが高かったりします。しかし次節で示すように、わずかな変更が大きな違いを生むことがあり、その結果はしばしば予測を超えます。本番環境での実験は、アイデアの価値を見極めるうえで強力な指針を与えてくれます。

本稿の貢献は以下のとおりです。
- 第3節では、Web環境における制御実験の概要をレビューし、統計的検出力やサンプルサイズといった、初学者向け解説ではしばしば省略されがちな重要項目を含む豊富な文献を紹介します。さらに実務で有効だった分散削減手法、実践上の拡張・制約も論じ、実務者が陥りやすい落とし穴を回避できるようにします。
- 第4節では、オンライン環境における多変量テスト（MVT）の代替策を複数提示します。ソフトウェア領域では、従来型MVTよりも並行した一因子テストのほうが適している場合がある理由を論じます。
- 第5節では、著者が関与してきた複数の実験システムを統合する汎用アーキテクチャを示し、その長所と短所を比較します。また、実際には統計的妥当性を担保する条件付き独立性テストに通らないランダム化・ハッシュ化スキームが存在することを明らかにします。
-第6節では、実務で得られた重要な教訓を共有します。

企業が実験基盤を構築すれば、テストコストや実験失敗のダメージは小さくなり、実験によるイノベーションが促進されます。「早く失敗する」ことで、アイデアの有効性を迅速に見極め、より成功し得る次のアイデアへと舵を切ることが可能になるのです。


# 2 Motivating examples

「事実が少なければ少ないほど、意見は強くなる」
― アーノルド・グラスオウ

以下の事例は、複数の領域で驚くべき結果をもたらしています。
	1.	最初の２つは、ほんの小さなUI変更が劇的な差を生んだ例です。
	2.	3番目の事例では、広告による短期的な収益向上とユーザー体験の劣化とのトレードオフを、制御実験でどのように評価・最適化したかを示します。
	3.	4番目の事例は、バックエンドアルゴリズム（この場合はAmazonの検索機能）に制御実験を適用した例です。

 ## 2.1 Checkout page at Doctor FootCare

 ECサイトのコンバージョン率とは、サイト訪問のうち購入に至った割合を指します。以下の例は Bryan Eisenberg の記事（Eisenberg 2003a, b）からのものです。

⸻

どちらのバリアントがより高いコンバージョン率を示すと思いますか？また、その差は統計的に有意だと思いますか？

図1に示す Doctor FootCare のチェックアウトページには、バリアントAとバリアントBで計9箇所の違いがあります。デザイナーがこれらを見せて「どちらを本番に出すべきか？」と尋ねられたら、あなたは正しく予測できるでしょうか？差がどれくらいあるか、またその差が有意かを見積もることができるでしょうか？

読者の皆さんにも、答えを読む前にぜひ考えてみてほしいと思います。どちらのバリアントが良いか、どの程度の差があるか、ぜひ予測してみてください。予測がいかに難しいかを実感できるはずです。

⸻

【実験結果】
バリアントAはバリアントBを10倍以上上回りました。実際にはサイト運営者がAからBに切り替えた結果、売上が90%も減少しています。ほとんどの変更点は良い方向でしたが、クーポンコード欄が致命的でした。ユーザーが「割引クーポンがあるのに自分が持っていないのでは？」と考え直してしまったためです。新バージョン（B）からクーポンコード欄を削除したところ、旧バージョン（A）と比べてコンバージョン率が6.5%向上しました。

## 2.2 Ratings of Microsoft Office help articles
Microsoft Office のヘルプを利用するユーザー（あるいは Office Online サイト（http://office.microsoft.com）を閲覧するユーザー）は、閲覧した記事を評価する機会が与えられます。初期実装では Yes/No ウィジェットが提示されていましたが、チームはこれを 5 つ星評価ウィジェットへと変更しました。

変更の動機
	1.	5 つ星評価は、より細かいフィードバックを提供し、コンテンツ制作者の評価に役立つ可能性がある。
	2.	5 つ星評価は、Yes/No 用と「理由」入力用の２つのポップアップを出す代わりに、単一のフィードバックボックスを提示することで、ユーザビリティが向上するはずである。

どちらのウィジェットの応答率※が高かったか予測できますか？
※ここで応答率とは、ウィジェットに対する何らかの操作（クリックや評価入力）を行った割合とする

驚くべきことに、5 つ星評価に切り替えたところ評価数は約 90％ 減少し、期待したユーザビリティ向上（動機 #2）を大きく逸脱しました。さらなるテストにより、２段階モデル（まず Yes/No 的な問い、次に星評価）を採用すると応答率が改善することが判明しました。具体的には、図 3 のように「Not helpful」から「Very helpful」までの 5 段階を明示した２段階モデルのウィジェットは、図 4 のシングルステージ型と比べ、応答率が 2.2 倍 に向上しました。

また、動機 #1 の「より細かいフィードバック」についても、多くのユーザーは 1 または 5 の極端な評価を選択し、中間の星をほとんど使いませんでした。助けが必要な状況では、「役に立ったか」「役に立たなかったか」の二択になりがちだったのです。

最終的にチームは、yes/no/I-don’t-know（わからない） という三択モデルを採用しました。この方式は単純な yes/no より応答率はやや下がったものの、追加の「わからない」という選択肢がもたらす情報が有用と判断されました。

## 2.3 MSN home page ads
多くのサイト運営者が直面する重要な問いは、広告をどれだけ掲載すべきかという点です。短期的には広告枠を増やすことで収益が向上しますが、特にターゲット外の広告の場合、それがユーザー体験にどのような影響を与えるかを見極めるのは難しいトレードオフです。この問いに直面したのが、2007年末のMicrosoft MSNホームページチームでした。

MSNホームページはモジュール構成になっており、ショッピングモジュールは「ファーストビュー」の右側に表示されています。提案では、このモジュールのすぐ下にさらに3件のオファーを追加するというものでした（図5）。ほとんどのユーザーにとって、それらは折り返し後の位置に表示されることになります。表示広告マーケティングチームの試算では、これらの追加オファーだけで一日数万ドルの収益が見込めるとされました。

ここでのチャレンジは、「広告収益」と「ユーザー体験の劣化」をどう比較・評価するかです。本稿第3.1節で取り上げた「OEC（Overall Evaluation Criterion：総合評価基準）」の事例です。今回は、ページビュー数とクリック数の変化を調べ、それぞれに金銭的価値を割り当てることにしました。（実験期間中、訪問頻度に統計的有意な変化は見られませんでした。）
- ページビューの価値：MSNホームページで表示される広告から得られる収益に基づく。
- クリックの価値：
1. MSNホームページから他のMSNネットワークサイト（MSN Autos、MSN Moneyなど）へ遷移したクリックに対し、その宛先サイトが評価する金銭的価値。これにより複数のページビューが生成される。
2. MSNホームページ経由ではなく検索エンジン広告（SEM）で獲得する同等のトラフィックを再獲得するために必要なクリック単価。ホームページ経由トラフィックが減少した場合、その「失われた」トラフィックをSEMで取り戻すコストです。

予想どおり、後者（#2）のSEMコストの方が高かったものの、直接収益以外の新規ユーザー獲得価値も考慮されており、両者はほぼ同等の評価額となりました。

実験はMSN USホームページのユーザーの5%を対象に12日間実施されました。その結果、クリック率は相対で0.38%減少し、統計的有意性（p=0.02）も確認されました。

この失われたクリック数を金銭的価値に換算したところ、予想される追加広告収益を上回ったため、ホームページへの広告追加案は見送られることとなりました。

## 2.4 Behavior-Based Search at Amazon
上記の事例はユーザーインターフェース（UI）の変更でしたが、ここではバックエンドのアルゴリズム変更に制御実験を適用した例をご紹介します。

2004年当時、著者の何人かが所属していた Amazon のデータマイニング＆パーソナライゼーション部門では、すでに「ある商品Xを購入した人は商品Yも購入している」という推薦アルゴリズム（購買ベース推薦）が確立されていました。これをさらに一般化し、「ある商品Xを閲覧した人は商品Yを購入している」「ある商品Xを閲覧した人は商品Yも閲覧している」というビューベース推薦も導入されていました。そこから新たに提案されたのが、「ある文字列Xで検索した人は商品Yを購入している」という行動ベース検索（Behavior-Based Search, BBS）です。UI上の表示には一切手を加えず、バックエンドの検索結果ランキングだけを変えることにより、検索結果の上位にこうした購買シグナルを反映させようというものでした。

たとえば、人が「24」というあいまいな検索語を入力したとき、従来の検索では『24 Italian Songs』のCDや24か月児向け衣料、24インチのタオルバーなど、さまざまな「24」を含む商品が返されてしまいます（この問題は検索語をユニークにする例外的な修飾語（例：「24 -foo」）を付けないかぎり、現在も同様に発生します）。一方 BBS アルゴリズムでは、「24」という検索を行った人が実際に購入したDVDボックスセットや関連書籍が上位に表示され、より適切な結果が得られます。アルゴリズムの強みは検索語の意味を理解せず、純粋に行動ログから学習する点ですが、その反面、検索語に含まれない商品が表示されることもあります。たとえば「Sony HD DVD Player」で検索すると、Sony製ではなく Toshiba製のHD DVDプレーヤーが上位に出てくることがあります。これは、SonyはBlu-rayプレーヤーを主に製造しており、多くのユーザーが「Sony HD DVD Player」で検索した後に Toshiba 製品を購入しているという行動シグナルによるものです。

こうした BBS の長所・短所を踏まえ、Amazon は制御実験を実施しました。その結果、2006年4月にワシントン大学 iEdge セミナーで公開されたところによると、この機能は Amazon の売上を約3%向上させ、数億ドル規模の増収をもたらしたと報告されています。

## 2.5 Other examples
これらは差分の大きさが極端な事例ですが、新しいデザインの成功を予測する難しさを如実に示しています。制御実験に関する emetrics 講演（Kohavi 2007）でも、さらに多くの事例が紹介されています。

優れた実験事例としては以下が挙げられます。
- Marketing Experiments 誌（McGlaughlin 2006）
- Design Choices Can Cripple a Website（Usborne 2005）
- Call to Action（Eisenberg & Eisenberg 2005）
- Which Sells Best（Eisenberg & Garcia 2006）

また、Forrester の Primer on A/B Testing（Chatham et al. 2004）には、ポジティブなROIを示す好例がいくつか載っています：
- Marriott は新しいオンライン予約フォームにより、追加で3,000万ドルの予約を獲得。
- Coach（ラグジュアリーアクセサリ小売業者）は、ベンダーに A/B テストで新検索エンジンの有効性を実証させることで、サイトの検索機能の効果を200%向上。
- Iomega（ディスクドライブメーカー）は、無料版ソフトウェアと製品版トライアルのどちらが好まれるか、どのメール用ランディングページが最良の転換率を生むかを実験的に検証し、キャンペーン成果を50%引き上げ。

さらに、Spool（2004）は Amtrak.com のサイトで登録成功率が4回に1回しかないことを指摘し、登録数が20%増えれば年間1,500万ドル以上の増収になると試算しています。
- InterContinental Hotels の A/B テストでは、検索結果に料金帯を追加して4,500万～6,000万ドルの追加予約を実現（Manning et al. 2006）。
- shop.com の The State of Retailing（Forrester Research 2005）では、米国137社の小売業者調査で「ユーザビリティテストとオファー・プロモーションの A/B テストを実施した小売業者の100%が、これらの手法を『効果的』または『非常に効果的』と評価」。
- 	Forrester の Web Analytics Spending Trends 2007（Burns 2006）では、ウェブ分析カテゴリで最も大幅な予算増加を見込んでいるのが A/B テストであり、主要予算増加を計画しているカテゴリは A/B テストと SEO/SEM の2つだけだったと報告しています。

# 3 Controlled experiments
「洗練された試行錯誤は、完璧な実行計画を立てることに勝る」
― IDEO 創業者 デイヴィッド・ケリー

「偉大なアイデアを得たければ、たくさんのアイデアを出しなさい」
― トーマス・エジソン

最も単純な制御実験（いわゆるA/Bテスト）では、ユーザーをランダムに二つのバリアントのいずれかに割り当てます：コントロール（A）またはトリートメント（B）です（図7参照；Mason et al. 1989; Box et al. 2005; Keppel et al. 1992）。

ここで重要なのは「ランダム」であることです。ユーザーを 「なんとなく」ばらまいてはいけません（Weiss 1997）。割り当てに影響を与える要因は一切許されません。収集された観測データに基づいて、各バリアントごとに総合評価基準（OEC: Overall Evaluation Criterion）を算出します（Roy 2001）。

たとえば、2.1節のチェックアウト事例では、OECとしてコンバージョン率、購入点数、収益、利益、顧客生涯価値の期待値、あるいはそれらを重み付けした指標などを用いることができます。得られたOECの差が統計的に有意かどうかを分析するわけです。

実験が適切に設計・実行されていれば、コントロールとトリートメントの差分以外に一貫した違いは存在しません。したがって、OECの差は必然的に割り当ての結果であり、因果関係が確立されます（Weiss 1997, p.215）。

ウェブ上での制御実験の基本的な解説書としては、Peterson (2004), Eisenberg & Eisenberg (2005), Chatham et al. (2004), Quarto-vonTivadar (2006), Miller (2006, 2007), Kaushik (2006), Peterson (2005), Tyler & Ledford (2006), Sterne (2002) などがあります。
<img width="670" height="470" alt="image" src="https://github.com/user-attachments/assets/8e740def-7124-4798-b95f-dfe673f064ae" />

概念自体は理解しやすく、基本的な考え方は多くの文献に通底していますが、ここで共有する重要な教訓はほとんど議論されることがありません。これらの教訓は、実験者が制御実験の適用範囲や限界を正しく理解し、結果を無効化してしまうようなミスを回避するのに役立ちます。

## 3.1 Terminology
御実験に関する用語は文献によって大きく異なります。以下では、本論文で使用する主要用語を定義し、一般に使われる代替用語を併記します。

Overall Evaluation Criterion（OEC）（Roy 2001）
実験の目的を定量化した評価尺度。統計学ではしばしば「応答変数（Response）」や「従属変数（Dependent Variable）」（Mason et al. 1989；Box et al. 2005）と呼ばれ、その他「アウトカム」「評価指標」「パフォーマンス指標」「適合度関数（Fitness Function）」（Quarto-vonTivadar 2006）などの呼称があります。複数の目的を持つ実験ではスコアカード方式を採用する場合もありますが（Kaplan & Norton 1996）、単一の指標（必要に応じて重み付きの組み合わせ）を選ぶことが強く推奨されます（Roy 2001, p.50）。単一指標により、複数実験間でのトレードオフを一度に決定でき、組織全体が明確な目的に沿って動けるようになります。なお、クリック数など短期的指標に偏らず、顧客生涯価値の予測や再訪率など長期的成果を予測する要因を含むOECが望ましいとされます（Ulwick 2005）。

Factor（要因）
OECに影響を与えると考えられる制御可能な実験変数。値（Value）を割り当て、これを「レベル（Level）」や「バージョン（Version）」とも呼びます。単純なA/Bテストでは要因は1つ、値はAとBの2つです。

Variant（バリアント）
要因のレベルを割り当てたユーザー体験のパターン。Control（既存版）またはTreatment（新バージョン）のいずれかです。「Treatment」と呼ぶ場合もありますが、本稿では既存版であるControlと、新たに試すTreatment variantsを区別して扱います。たとえばバグ発生時には実験を中止し、すべてのユーザーにControlを提示します。

Experimental unit（実験単位）
各バリアントで計測した指標を集計する対象単位。独立とみなされる「項目（item）」とも呼ばれます。ウェブ実験では多くの場合ユーザーですが、ユーザー日（user-day）、セッション、ページビューを単位とする場合もあります。いずれにせよ、ランダム割り当てはユーザー単位が望ましく、実験中一貫した体験を提供するためにユーザーIDをクッキーに保存して割り当てを行います（ユーザー単位以外のランダム化が適切な場合については付録参照）。

Null hypothesis（帰無仮説）
バリアント間でOECに差がなく、実験中に観測される違いはランダム揺らぎによるという仮説（H₀）。

Confidence level（信頼水準）
帰無仮説が真であるときに、棄却せずに維持する確率。

Power（検出力）
帰無仮説が偽であるときに、正しく棄却できる確率。差が実際に存在する場合に検出できる能力を表します。

A/A test（A/Aテスト）
「Null Test」とも呼ばれ（Peterson 2004）、A/Bテストと同じ要領でユーザーを2グループに割り当てるものの、両群に同一体験（両方ともControl）を提供します。
1. 検出力計算のためのデータ収集・ばらつき評価
2. 実験環境の検証（信頼水準95%ならば約5%の確率で帰無仮説が誤って棄却されるはず）

Standard deviation（標準偏差, Std-Dev）
ばらつきの指標。通常σで表されます。

Standard error（標準誤差, Std-Err）
サンプル統計量の標本分布の標準偏差（Mason et al. 1989）。独立観測値nの平均の標準誤差は σ̂/√n（σ̂は標本標準偏差）で計算されます。

## 3.2 Hypothesis testing and sample size

Treatment群がControl群と異なるかどうかを評価するには、統計的検定を行います。OECに差があると帰無仮説（OECに差はない）が棄却された場合、そのTreatmentは統計的に有意な差があると判断します。

検定の詳細は多くの統計書（Mason et al. 1989; Box et al. 2005; Keppel et al. 1992）に譲りますが、検定結果に影響を与える主な要因は以下のとおりです。
1. 信頼水準 (Confidence level)
通常95%に設定します。これは「差がないにもかかわらず差があると誤判断する確率」（第一種過誤）が5%であることを意味します。信頼水準を上げると、他の条件が同じでも検出力は低下します。
1. 検出力 (Power)
帰無仮説が偽（実際にOECに差がある）場合に、正しく棄却できる確率です。一般に80～95%程度が望まれますが、直接制御はできません。（帰無仮説を棄却せずに維持してしまう誤りが第二種過誤です。）
1. 標準誤差 (Standard error)



小さいほど検定の検出力が高まります。標準誤差を減らすには主に以下の3つの方法があります：

1. サンプルサイズを増やす
OECは大規模サンプルの平均であることが多く、平均の標準誤差はサンプルサイズの平方根に反比例します。したがって、実験を長く実施してサンプル数を増やせば標準誤差は減少し、検出力が向上します（Sect. 3.2.1 の例参照）。
1. ばらつきの小さいOEC成分を使う
標準偏差 σ が小さい指標を選ぶと、結果として標準誤差も小さくなります。たとえば、コンバージョン確率（0–100%）は購入単位数（小さな整数値）より標準偏差が小さく、購入単位数は収益（実数値）より小さい傾向があります（Sect. 3.2.1 の例参照）。
1. 不要なユーザーを除外してばらつきを抑える
たとえば、チェックアウトページの変更を評価する場合、そのページに到達したユーザーのみを対象に分析し、到達していないユーザーをOEC計算から除外すると、ノイズが減って標準誤差が下がります（Sect. 3.2.3 の例参照）。
1. 効果量 (Effect)
バリアント間のOECの差、すなわち Treatment 群の平均 – Control 群の平均 です。差が大きいほど検出が容易になるため、優れたアイデアは見逃されにくくなります。一方で、効果量が小さい場合は第二種過誤（本当は差があるのに見逃す）が起こりやすくなります。

ここで役立つ式を2つ紹介します。まずは、A/Bテスト（単因子仮説検定）で用いられるt検定の統計量です

### 3.2.1 Example: impact of lower-variability OEC on the sample size
この例では、同じ実験でも目的とする指標（OEC）を変えるだけで、必要なサンプルサイズが大きく変わることを示しています。
	•	売上（Revenue）をOECとした場合
	•	平均支出 $3.75、標準偏差 $30
	•	5% の変化を検出するには約 409,000 ユーザーが必要
	•	実験期間にして約 6 週間かかる想定
	•	コンバージョン率（購入イベント）をOECとした場合
	•	購入確率 p=0.05 のベルヌーイ試行としてモデル化
	•	標準偏差 √(p(1–p))
	•	5% の変化を検出するには約 122,000 ユーザーで十分
	•	実験期間にして約 2 週間で済む想定

つまり、ばらつきの小さい指標（ここではコンバージョン率）をOECに選ぶことで、サンプルサイズを3.3倍削減でき、実験の実行時間やコストを大幅に短縮できます。

⸻

この考え方を踏まえると、実際のABテスト設計では
1. OECの選定：短期収益ではなく、ばらつきの小さい指標（例：コンバージョン率、クリック率など）を検討する。
1. サンプルサイズ計算：検出したい効果量（ここでは5%）に合わせて、必要ユーザー数を計算する。
1. 実験期間の見積もり：自サイトのトラフィック量から、何日／週で必要サンプルに到達するかを算出し、運用計画に反映する。

といったステップを踏むのが有効です。もし別の効果量や指標での必要サンプル数を計算したい、あるいはPythonで自動計算スクリプトを組んでみたい、など具体的なご要望があればお知らせください。


### 3.2.2 Example: impact of reduced sensitivity on the sample size
感度（δ）はサンプルサイズの式で二乗して現れるため、たとえばコンバージョン率の変化を5％から20％に検出しやすくする（感度を4倍にする）と、必要なユーザー数は16分の1に減り、約7,600ユーザーで足ります。後述するように、これが実装上のバグを素早く検出できる理由です。

たとえば本来はOECの1％変化を検出するよう実験を設計していても、実装ミスでユーザーが悪い体験をしOECが20％も低下した場合、そのバグは計画期間の1/20ではなく1/400の時間で検出できます。もし実験を2週間行う予定なら、最初の1時間で重大な問題を見つけられるのです。

### 3.2.3 Example: filtering users not impacted by the change
チェックアウトプロセスに変更を加えた場合、差分を確認できずノイズだけを増やしてしまうユーザーを除外するために（3.c）、チェックアウトを「開始した」ユーザーのみを分析すべきです。仮に実験期間中のユーザーの10％がチェックアウトを開始し、そのうち50％が完了するとします。このセグメントはより均質になるため、OECのばらつき（標準偏差）が低くなります。先ほどと同じ前提を使うと、平均コンバージョン率は0.5、標準偏差も0.5となり、5％の変化を検出するにはチェックアウトを完了したユーザー6,400人で十分です

### 3.2.4 The choice of OEC must be made in advance
## 3.3 Confidence intervals for absolute and percent effect
### 3.3.1 Confidence intervals for absolute effect
### 3.3.2 Confidence intervals for percent effect

## 3.4 Effect of robots on experimental results
ロボットは推定値に著しい歪みをもたらし、実験の前提を無効化するほどの影響を与えることがあります。たとえば、A/Aテストにおいて本来有意でないはずの多くの指標が、ロボットの影響で偽陽性率が5％を大幅に超えて有意になってしまうケースを確認しています。実験を行う上では、特にユーザーIDと相互作用するタイプのロボットを除外することが重要です。あるウェブサイトでは、ロボットがサイト全体のページビューの最大半数を占めると推定されています（Kohavi et al. 2004）。多くのロボットは人間ユーザーと同様のアクセスパターンを示すため、両者を明確に区別するのは困難です。単純なロボットであれば User-Agent や IP アドレスなどの基本的な特徴でフィルタリングできますが、多くの最新ロボットは高度な手法を使って検出やフィルタを回避します（Tan and Kumar 2002）。

### 3.4.1 JavaScript versus server-side call
ロボットは推定値に著しい歪みをもたらし、実験の前提を無効化するほどの影響を与えることがあります。たとえば、A/Aテストで本来有意でないはずのメトリクスが、ロボットの影響により偽陽性率が5％を大きく超えて有意になってしまう事例も観測されています。実験を行う際には、特にユーザーIDと相互作用するタイプのロボットを除外することが重要です。あるウェブサイトでは、ロボットが全ページビューの半数近くを占めると推定されています（Kohavi et al. 2004）。多くのロボットは人間ユーザーと同様のアクセスパターンを示すため、両者を明確に区別するのは困難です。ベーシックなロボット（いわゆる“ベニン”なロボット）は User-Agent や IP アドレスなどの基本的特徴でフィルタ可能ですが、多くの最新ロボットは検出やフィルタリングを回避する高度な手法を用いています（Tan and Kumar 2002）。

### 3.4.2 Robots that reject cookies
未識別のリクエストは分析から除外することを推奨します。これにより、クッキーを拒否するロボットは実験結果に含まれなくなります。つまり、処置の割り当てやユーザー行動のデータ収集を、ユーザーのクッキーにユーザーIDが保存されている訪問者に限定すれば、これらのロボットはユーザー数にも行動データにも加えられません。

### 3.4.3 Robots that accept cookies
ロボットがクッキーを受け入れ、かつそれを削除しない場合、その影響は非常に大きくなることがあります。特に、サイト上で大量の操作を行うロボットは、Treatment 群や Control 群への混入が比較結果を著しく歪める恐れがあります。実際、あるロボットが１時間で同一ページに7,000回もクリックを行ったり、１日で3,000回以上のページビューを生成したりする例を確認しています。こうしたロボットが存在する状態で Treatment と Control を比較する仮説検定は、非常に誤解を招きやすくなります。これらのロボットは効果推定をバイアスさせるだけでなく、多くの指標の標準偏差を増大させるため、検出力も低下させます。

したがって、クッキーを削除せず、かつ単一のユーザーIDに対して大量の操作（ページビューや、onclick ハンドラによるクリックなど）を行うロボットは、積極的に除外する必要があります。一方、クッキーを受け入れない、あるいは一度あるいはごく少数の操作後にクッキーをクリアするロボットは、Treatment と Control の比較にはほとんど影響を与えません。ロボット除外は、既知のロボットの User-Agent を持つユーザーを除外リストに載せる方法と、ウェブサイトごとに異なるヒューリスティック（Kohavi 2003）を組み合わせることで実現できます。

## 3.5 Extensions for online settings


### 3.5.1 Treatment ramp-up
実験は、まずごく少数のユーザーにだけTreatmentを割り当てるところから始め、その後その割合を段階的に引き上げることができます。たとえば、最終的にA/Bテストを50%/50%で実施する場合、最初は99.9%/0.1%で始め、Treatmentの割合を0.1%→0.5%→2.5%→10%→50%と順に増やしていきます。各ステップは数時間程度実行し、その都度データを分析して、より多数のユーザーに公開する前にTreatmentに重大な問題がないかを確認します。検出力の式に現れる“割合の二乗”の効果により、少数のサンプルでも大きな異常を迅速に検知できるため、多くのユーザーが問題のあるTreatmentにさらされる前に実験を中止できるのです。

### 3.5.2 Automation

組織が明確なOECを持っていれば、自動化された探索に適した領域を最適化する実験を実行できます。たとえば、Amazon のホームページのスロットは自動的に最適化されています（Kohavi et al. 2004）。迅速な意思決定が求められる場合（例えばポータルサイトの見出し最適化など）は、誤りのコストが低いため、信頼水準を低めに設定してもかまいません。こうした最適化には、マルチアーム・バンディット・アルゴリズム（Wikipedia 2008）やホーフディング・レース（Maron and Moore 1994）などを利用できます。

### 3.5.3 Software migrations
実験はソフトウェアのマイグレーションにも役立てることができます。機能やシステムを新しいバックエンド、新しいデータベース、あるいは新しいプログラミング言語に移行する場合で、ユーザーに見える機能に変更がないことが期待されるなら、A/Bテストを実行して「バリアント間に差がない」という帰無仮説を維持することを目標にできます。

こうしたマイグレーションを複数回経験してきましたが、移行作業が完了と宣言された後でも、A/Bテストで主要な指標に有意な差異があることが判明し、移植時のバグを特定する助けとなったケースがありました。

ここでの目標は帰無仮説を維持することにあります。そのため、帰無仮説が偽である場合に確実に棄却できるだけの十分な検出力（Power）を実験に持たせることが極めて重要です。

## 3.6 Limitations

# 4 MultiVariable Testing

## 4.1 Traditional MVT

## 4.2 MVT by running concurrent tests

## 4.3 Overlapping experiments

# 5 Implementation architecture

## 5.1 Randomization algorithm

### 5.1.1 Pseudorandom with caching

### 5.1.2 Hash and partition

## 5.2 Assignment method

### 5.2.1 Traffic splitting

### 5.2.2 Page rewriting

### 5.2.3 Client-side assignment


### 5.2.4 Server-side assignment

### 5.2.5 Summary

### 5.3 Data path

### 5.3.1 Event-triggered filtering

### 5.3.2 Raw data collection

# 6 Lessons learned
「理論と実践のギャップは、理論上で想定されるギャップよりも、実際の現場でのギャップのほうがはるかに大きい」
― ヤン・L.A.・ファン・デ・スネプスホイト

# 7 Summary
「ほとんどあらゆる疑問は、テストキャンペーンによって安価に、迅速に、そして最終的に解決できる。そしてそれが答えを出す方法なのだ――会議室で議論するのではなく。最後の審判の場へ行け――あなたの商品を買ってくれる人々のもとへ。」
― クロード・ホプキンス『Scientific Advertising』, 1923年

「ウェブベースのアプリケーションにおいて、容易に実験できる能力は極めて重要だ。オンラインの世界は決して静止しない。新しいユーザー、新しい製品、新しい技術が絶えず流入している。何がうまく機能し、何が機能しないかを迅速に見定めることが、生き残りか絶滅かを分けるのだ。」
― ハル・ヴァリアン, 2007年

従来の知識発見（Knowledge Discovery）やデータマイニングは洞察を提供しますが、発見されるパターンは相関関係にとどまり、“リーク”（情報漏洩）によって生じたものと、有用かつ実行可能なパターンとを切り分けるのが難しいという課題があります（Kohavi et al. 2004）。一方、制御実験ではランダム割り当てによって交絡変数をすべてのバリアントに均等に分散させるため、異なるバリアント間で行った変更と、Overall Evaluation Criterion（OEC）を含む評価指標との因果関係を確立できます（Keppel et al. 1992）。このような環境下でデータマイニング技術を併用すると、たとえば制御実験で導入した機能の恩恵を受けるユーザーセグメントを特定するといった、機能改善やパーソナライゼーションの好循環を生む極めて有益な洞察を得ることが可能になります。

基本的な制御実験の考え方自体は理解しやすいものの、ウェブ向けに体系的にまとめられた解説はこれまで存在しませんでした。本稿では、汎用的なアーキテクチャ、段階的導入と自動中断、実践上のランダム化・ハッシュ化の問題点、そしてOECにまつわる組織的課題など、新たに得られた教訓や洞察を随所で共有しています。

今日の製品におけるソフトウェア機能は、第二次世界大戦前の医療における処方と同様に、専門家の判断に基づいて決められることが一般的でした。しかし、オンラインで顧客行動を直接観察できる現代では、もっと良いやり方が可能です。Marks 著『The Progress of Experiment: Science and Therapeutic Reform in the United States, 1900–1990』（2000年、p. 3）には、医療知識の発展における計画実験の重要性が次のように述べられています：

「世紀の後半の改革者たちは、経験豊富な臨床医の判断への信頼を捨て去った。その代わりに、無作為化二重盲検対照試験という、感情を排した科学的厳格さの基準を提示した。」

多くの組織では、強い意見を持つマネージャーがデータを持たないまま意思決定を行います。そこで私たちは“HiPPO（Highest Paid Person’s Opinion）＝最高報酬者の意見”という言葉を使い、最終的にモノを言うのはユーザーの評価であることを常に意識するようにしています。一部の著者は実験手法を「イノベーションへの新たな必須条件（New Imperative for Innovation）」（Thomke 2001）と呼び、「新しい技術のおかげで、これまで以上に迅速かつ安価に複雑な実験が行えるようになっている」と指摘しています。私たちも同感であり、顧客体験こそが最終的に重要である以上、常に実験を通じてユーザーの声に耳を傾けることで、企業はイノベーションを加速できると考えています。
